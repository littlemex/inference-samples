{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to /home/ubuntu/.config/pip/pip.conf\r\n"
     ]
    }
   ],
   "source": [
    "# torch-neuronx requires 1.13.1 or more above\n",
    "\n",
    "!pip install -U pip --quiet\n",
    "#!pip install --upgrade google-auth --quiet\n",
    "\n",
    "!pip install protobuf==3.19.6 --quiet\n",
    "!pip install pytorch-lightning==1.6.4 --quiet\n",
    "!pip install transformers==4.11.3 --quiet\n",
    "!pip install torchvision==0.14.1 --quiet\n",
    "!pip install torch==1.13.1 --quiet\n",
    "!pip install timm==0.5.4 --quiet\n",
    "!pip install donut-python==1.0.9 --quiet\n",
    "!pip config set global.extra-index-url https://pip.repos.neuron.amazonaws.com\n",
    "!pip install neuronx-cc==2.* torch-neuronx --quiet\n",
    "\n",
    "!pip install PyYAML==5.4.1 --quiet\n",
    "!pip install --upgrade requests chardet --quiet\n",
    "!pip install --upgrade urllib3==1.26.7 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Desired=Unknown/Install/Remove/Purge/Hold\r\n",
      "| Status=Not/Inst/Conf-files/Unpacked/halF-conf/Half-inst/trig-aWait/Trig-pend\r\n",
      "|/ Err?=(none)/Reinst-required (Status,Err: uppercase=bad)\r\n",
      "||/ Name                    Version            Architecture Description\r\n",
      "+++-=======================-==================-============-=================================\r\n",
      "ii  aws-neuronx-runtime-lib 2.13.6.0-29de104d6 amd64        neuron_runtime built using CMake\r\n"
     ]
    }
   ],
   "source": [
    "!dpkg -l aws-neuronx-runtime-lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aws-neuronx-runtime-discovery 2.9\n",
      "donut-python                  1.0.9\n",
      "libneuronxla                  0.5.207\n",
      "neuronx-cc                    2.6.0.19+3d819e565\n",
      "neuronx-hwm                   2.6.0.0+826e77395\n",
      "pytorch-lightning             1.6.4\n",
      "torch                         1.13.1\n",
      "torch-neuronx                 1.13.1.1.7.0\n",
      "torch-xla                     1.13.1+torchneuron6\n",
      "torchmetrics                  0.11.4\n",
      "torchvision                   0.14.1\n",
      "transformers                  4.11.3\n"
     ]
    }
   ],
   "source": [
    "!pip list |grep -e torch -e neuron -e donut -e transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: Python 3.8.10\n",
      "\n",
      "CUDA version: 11.8\n",
      "\n",
      "torch version: 1.13.1+cu117\n",
      "\n",
      "transformers version: 4.11.3\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import subprocess\n",
    "\n",
    "# 現在の日時を取得\n",
    "now = datetime.datetime.now()\n",
    "date_str = now.strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "# Pythonバージョンの確認\n",
    "python_version = subprocess.run(['python3.8', '--version'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout.strip()\n",
    "\n",
    "# CUDAバージョンの確認\n",
    "try:\n",
    "    nvcc_version = subprocess.run(['nvcc', '--version'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout.strip()\n",
    "    _, cuda_version = nvcc_version.split('\\n')[-2].split(',')[1].split()\n",
    "except:\n",
    "    cuda_version = 'CPU'\n",
    "\n",
    "# torchバージョンの確認\n",
    "import torch\n",
    "torch_version = torch.__version__\n",
    "\n",
    "# transformersバージョンの確認\n",
    "import transformers\n",
    "transformers_version = transformers.__version__\n",
    "\n",
    "# 実行時のコマンドと実行結果を保存するファイル名\n",
    "log_file = f\"diffusion_{date_str}.log\"\n",
    "\n",
    "\n",
    "print(f\"Python version: {python_version}\\n\")\n",
    "print(f\"CUDA version: {cuda_version}\\n\")\n",
    "print(f\"torch version: {torch_version}\\n\")\n",
    "print(f\"transformers version: {transformers_version}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import sys\n",
    "from donut import DonutModel\n",
    "\n",
    "import IPython\n",
    "import pprint\n",
    "from donut.model import DonutConfig, DonutModel, SwinEncoder, BARTDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/clovaai/donut/blob/a0e94bf145d81cfc934eae8848f1269ae9ca46a2/donut/model.py#L11\n",
    "import math\n",
    "import os\n",
    "import re\n",
    "from typing import Any, List, Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "import PIL\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from PIL import ImageOps\n",
    "from timm.data.constants import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
    "from timm.models.swin_transformer import SwinTransformer\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms.functional import resize, rotate\n",
    "from transformers import MBartConfig, MBartForCausalLM, XLMRobertaTokenizer\n",
    "from transformers.file_utils import ModelOutput\n",
    "from transformers.modeling_utils import PretrainedConfig, PreTrainedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_neuronx\n",
    "import os\n",
    "import numpy as np\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import sys\n",
    "\n",
    "import IPython\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from donut import DonutModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-06-02 12:25:45--  https://media.slidesgo.com/storage/162635/conversions/1-market-share-infographics-thumb.jpg\n",
      "Resolving media.slidesgo.com (media.slidesgo.com)... 172.67.9.18, 104.22.0.146, 104.22.1.146, ...\n",
      "Connecting to media.slidesgo.com (media.slidesgo.com)|172.67.9.18|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 23066 (23K) [image/jpeg]\n",
      "Saving to: ‘./test_01.jpg’\n",
      "\n",
      "./test_01.jpg       100%[===================>]  22.53K  --.-KB/s    in 0s      \n",
      "\n",
      "2023-06-02 12:25:45 (205 MB/s) - ‘./test_01.jpg’ saved [23066/23066]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!rm -rf test_01.jpg\n",
    "\n",
    "# 英語のスライド\n",
    "!wget -c https://media.slidesgo.com/storage/162635/conversions/1-market-share-infographics-thumb.jpg \\\n",
    "      -O ./test_01.jpg\n",
    "\n",
    "# 日本語のレシート\n",
    "# !wget -c https://www.isp21.co.jp/wp-content/uploads/solution/library/library02-1.jpg \\\n",
    "#       -O ./test_01.jpg\n",
    "\n",
    "input_img_path = './test_01.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6657f04ad3d844d2a671355361d56882",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/401 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ec96d68fd8f4610b101bce4116d7bcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/965M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/aws_neuron_venv_pytorch/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef770a4e1996449499e2a66c15731197",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "783a7bde5f3d4abaac6bd081b50e3b74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/558 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86baa0776daf4db88aaf5f8ae3610c84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/747 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3088ee2e33f443593f17174f5405987",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/536 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e0ed2d50f5e45b18aa6a778bf7b254d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/3.83M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DonutModel(\n",
       "  (encoder): SwinEncoder(\n",
       "    (model): SwinTransformer(\n",
       "      (patch_embed): PatchEmbed(\n",
       "        (proj): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "      (layers): Sequential(\n",
       "        (0): BasicLayer(\n",
       "          dim=128, input_resolution=(640, 480), depth=2\n",
       "          (blocks): ModuleList(\n",
       "            (0): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): Identity()\n",
       "              (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (1): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath()\n",
       "              (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (downsample): PatchMerging(\n",
       "            input_resolution=(640, 480), dim=128\n",
       "            (reduction): Linear(in_features=512, out_features=256, bias=False)\n",
       "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicLayer(\n",
       "          dim=256, input_resolution=(320, 240), depth=2\n",
       "          (blocks): ModuleList(\n",
       "            (0): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath()\n",
       "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (1): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath()\n",
       "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (downsample): PatchMerging(\n",
       "            input_resolution=(320, 240), dim=256\n",
       "            (reduction): Linear(in_features=1024, out_features=512, bias=False)\n",
       "            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (2): BasicLayer(\n",
       "          dim=512, input_resolution=(160, 120), depth=14\n",
       "          (blocks): ModuleList(\n",
       "            (0): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath()\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (1): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath()\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (2): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath()\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (3): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath()\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (4): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath()\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (5): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath()\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (6): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath()\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (7): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath()\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (8): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath()\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (9): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath()\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (10): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath()\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (11): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath()\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (12): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath()\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (13): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath()\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (downsample): PatchMerging(\n",
       "            input_resolution=(160, 120), dim=512\n",
       "            (reduction): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "            (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (3): BasicLayer(\n",
       "          dim=1024, input_resolution=(80, 60), depth=2\n",
       "          (blocks): ModuleList(\n",
       "            (0): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath()\n",
       "              (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (1): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath()\n",
       "              (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (avgpool): AdaptiveAvgPool1d(output_size=1)\n",
       "      (head): Identity()\n",
       "    )\n",
       "  )\n",
       "  (decoder): BARTDecoder(\n",
       "    (model): MBartForCausalLM(\n",
       "      (model): MBartDecoderWrapper(\n",
       "        (decoder): MBartDecoder(\n",
       "          (embed_tokens): Embedding(57544, 1024, padding_idx=1)\n",
       "          (embed_positions): MBartLearnedPositionalEmbedding(10, 1024)\n",
       "          (layers): ModuleList(\n",
       "            (0): MBartDecoderLayer(\n",
       "              (self_attn): MBartAttention(\n",
       "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (encoder_attn): MBartAttention(\n",
       "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (1): MBartDecoderLayer(\n",
       "              (self_attn): MBartAttention(\n",
       "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (encoder_attn): MBartAttention(\n",
       "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (2): MBartDecoderLayer(\n",
       "              (self_attn): MBartAttention(\n",
       "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (encoder_attn): MBartAttention(\n",
       "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (3): MBartDecoderLayer(\n",
       "              (self_attn): MBartAttention(\n",
       "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (encoder_attn): MBartAttention(\n",
       "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (lm_head): Linear(in_features=1024, out_features=57544, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "donut_model = DonutModel.from_pretrained(\"naver-clova-ix/donut-base-finetuned-rvlcdip\").to(\"cpu\")\n",
    "donut_model.eval() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_prompt = \"<s_rvlcdip>\"\n",
    "\n",
    "input_img = Image.open(\"./test_01.jpg\")\n",
    "output = donut_model.inference(image=input_img, prompt=task_prompt)[\"predictions\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'class': 'presentation'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SwinTransformer() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_base_patch4_window12_384_22kto1k.pth\" to /home/ubuntu/.cache/torch/hub/checkpoints/swin_base_patch4_window12_384_22kto1k.pth\n"
     ]
    }
   ],
   "source": [
    "swin_state_dict = timm.create_model(\"swin_base_patch4_window12_384\", pretrained=True).state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_swin_state_dict = model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(new_swin_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timm.models.swin_transformer import WindowAttention, SwinTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_attention = WindowAttention(dim=1024, num_heads=32, window_size=(10, 10), qkv_bias=True, attn_drop=0.0, proj_drop=0.0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = torch.zeros([48, 100, 1024])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0102, -0.0268, -0.0262,  ..., -0.0072, -0.0084,  0.0367],\n",
       "         [ 0.0102, -0.0268, -0.0262,  ..., -0.0072, -0.0084,  0.0367],\n",
       "         [ 0.0102, -0.0268, -0.0262,  ..., -0.0072, -0.0084,  0.0367],\n",
       "         ...,\n",
       "         [ 0.0102, -0.0268, -0.0262,  ..., -0.0072, -0.0084,  0.0367],\n",
       "         [ 0.0102, -0.0268, -0.0262,  ..., -0.0072, -0.0084,  0.0367],\n",
       "         [ 0.0102, -0.0268, -0.0262,  ..., -0.0072, -0.0084,  0.0367]],\n",
       "\n",
       "        [[ 0.0102, -0.0268, -0.0262,  ..., -0.0072, -0.0084,  0.0367],\n",
       "         [ 0.0102, -0.0268, -0.0262,  ..., -0.0072, -0.0084,  0.0367],\n",
       "         [ 0.0102, -0.0268, -0.0262,  ..., -0.0072, -0.0084,  0.0367],\n",
       "         ...,\n",
       "         [ 0.0102, -0.0268, -0.0262,  ..., -0.0072, -0.0084,  0.0367],\n",
       "         [ 0.0102, -0.0268, -0.0262,  ..., -0.0072, -0.0084,  0.0367],\n",
       "         [ 0.0102, -0.0268, -0.0262,  ..., -0.0072, -0.0084,  0.0367]],\n",
       "\n",
       "        [[ 0.0102, -0.0268, -0.0262,  ..., -0.0072, -0.0084,  0.0367],\n",
       "         [ 0.0102, -0.0268, -0.0262,  ..., -0.0072, -0.0084,  0.0367],\n",
       "         [ 0.0102, -0.0268, -0.0262,  ..., -0.0072, -0.0084,  0.0367],\n",
       "         ...,\n",
       "         [ 0.0102, -0.0268, -0.0262,  ..., -0.0072, -0.0084,  0.0367],\n",
       "         [ 0.0102, -0.0268, -0.0262,  ..., -0.0072, -0.0084,  0.0367],\n",
       "         [ 0.0102, -0.0268, -0.0262,  ..., -0.0072, -0.0084,  0.0367]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.0102, -0.0268, -0.0262,  ..., -0.0072, -0.0084,  0.0367],\n",
       "         [ 0.0102, -0.0268, -0.0262,  ..., -0.0072, -0.0084,  0.0367],\n",
       "         [ 0.0102, -0.0268, -0.0262,  ..., -0.0072, -0.0084,  0.0367],\n",
       "         ...,\n",
       "         [ 0.0102, -0.0268, -0.0262,  ..., -0.0072, -0.0084,  0.0367],\n",
       "         [ 0.0102, -0.0268, -0.0262,  ..., -0.0072, -0.0084,  0.0367],\n",
       "         [ 0.0102, -0.0268, -0.0262,  ..., -0.0072, -0.0084,  0.0367]],\n",
       "\n",
       "        [[ 0.0102, -0.0268, -0.0262,  ..., -0.0072, -0.0084,  0.0367],\n",
       "         [ 0.0102, -0.0268, -0.0262,  ..., -0.0072, -0.0084,  0.0367],\n",
       "         [ 0.0102, -0.0268, -0.0262,  ..., -0.0072, -0.0084,  0.0367],\n",
       "         ...,\n",
       "         [ 0.0102, -0.0268, -0.0262,  ..., -0.0072, -0.0084,  0.0367],\n",
       "         [ 0.0102, -0.0268, -0.0262,  ..., -0.0072, -0.0084,  0.0367],\n",
       "         [ 0.0102, -0.0268, -0.0262,  ..., -0.0072, -0.0084,  0.0367]],\n",
       "\n",
       "        [[ 0.0102, -0.0268, -0.0262,  ..., -0.0072, -0.0084,  0.0367],\n",
       "         [ 0.0102, -0.0268, -0.0262,  ..., -0.0072, -0.0084,  0.0367],\n",
       "         [ 0.0102, -0.0268, -0.0262,  ..., -0.0072, -0.0084,  0.0367],\n",
       "         ...,\n",
       "         [ 0.0102, -0.0268, -0.0262,  ..., -0.0072, -0.0084,  0.0367],\n",
       "         [ 0.0102, -0.0268, -0.0262,  ..., -0.0072, -0.0084,  0.0367],\n",
       "         [ 0.0102, -0.0268, -0.0262,  ..., -0.0072, -0.0084,  0.0367]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window_attention.forward(dummy_input) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowAttention(\n",
       "  original_name=WindowAttention\n",
       "  (qkv): Linear(original_name=Linear)\n",
       "  (attn_drop): Dropout(original_name=Dropout)\n",
       "  (proj): Linear(original_name=Linear)\n",
       "  (proj_drop): Dropout(original_name=Dropout)\n",
       "  (softmax): Softmax(original_name=Softmax)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.jit.trace(window_attention, dummy_input) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuronModule(original_name=NeuronModule)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_neuronx.trace(window_attention, dummy_input) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timm.models.swin_transformer import SwinTransformerBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "stb = SwinTransformerBlock(dim=1024, input_resolution=(80, 60), num_heads=32, window_size=10, shift_size=5, mlp_ratio=4.0, qkv_bias=True, drop=0.0, attn_drop=0.0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = torch.zeros((1, 4800, 1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2312,  0.1291,  0.0728,  ..., -0.0651,  0.0243, -0.0362],\n",
       "         [ 0.2312,  0.1291,  0.0728,  ..., -0.0651,  0.0243, -0.0362],\n",
       "         [ 0.2312,  0.1291,  0.0728,  ..., -0.0651,  0.0243, -0.0362],\n",
       "         ...,\n",
       "         [ 0.2312,  0.1291,  0.0728,  ..., -0.0651,  0.0243, -0.0362],\n",
       "         [ 0.2312,  0.1291,  0.0728,  ..., -0.0651,  0.0243, -0.0362],\n",
       "         [ 0.2312,  0.1291,  0.0728,  ..., -0.0651,  0.0243, -0.0362]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stb(dummy_input) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/aws_neuron_venv_pytorch/lib/python3.8/site-packages/torch/__init__.py:853: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert condition, message\n",
      "/home/ubuntu/aws_neuron_venv_pytorch/lib/python3.8/site-packages/timm/models/swin_transformer.py:119: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  B = int(windows.shape[0] / (H * W / window_size / window_size))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SwinTransformerBlock(\n",
       "  original_name=SwinTransformerBlock\n",
       "  (norm1): LayerNorm(original_name=LayerNorm)\n",
       "  (attn): WindowAttention(\n",
       "    original_name=WindowAttention\n",
       "    (qkv): Linear(original_name=Linear)\n",
       "    (attn_drop): Dropout(original_name=Dropout)\n",
       "    (proj): Linear(original_name=Linear)\n",
       "    (proj_drop): Dropout(original_name=Dropout)\n",
       "    (softmax): Softmax(original_name=Softmax)\n",
       "  )\n",
       "  (drop_path): Identity(original_name=Identity)\n",
       "  (norm2): LayerNorm(original_name=LayerNorm)\n",
       "  (mlp): Mlp(\n",
       "    original_name=Mlp\n",
       "    (fc1): Linear(original_name=Linear)\n",
       "    (act): GELU(original_name=GELU)\n",
       "    (drop1): Dropout(original_name=Dropout)\n",
       "    (fc2): Linear(original_name=Linear)\n",
       "    (drop2): Dropout(original_name=Dropout)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.jit.trace(stb, dummy_input) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuronModule(original_name=NeuronModule)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_neuronx.trace(stb, dummy_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "swin_transformer = SwinTransformer() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = torch.zeros((1, 3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-8.3674e-01,  8.7812e-01, -2.2818e-01, -1.1928e-01, -1.7823e-02,\n",
       "          9.7244e-02, -9.4447e-02,  5.2866e-01,  9.6839e-01,  3.7772e-01,\n",
       "          1.9505e-01,  2.6907e-01, -3.7482e-01,  5.7078e-01,  2.9605e-01,\n",
       "         -1.0211e-01,  5.4349e-01,  1.1274e+00,  9.0265e-01,  4.0115e-01,\n",
       "         -7.4968e-02,  1.1743e-01,  6.4910e-01, -2.7082e-01, -3.8752e-01,\n",
       "          3.7325e-01,  1.1855e+00, -4.5698e-01,  2.4166e-01,  9.5284e-01,\n",
       "         -7.8355e-02,  2.0212e-01,  4.0976e-01,  2.2871e-01, -3.5197e-01,\n",
       "         -4.6748e-01, -3.6364e-01,  1.0386e+00,  2.3065e-02,  1.9896e-01,\n",
       "         -7.2755e-02,  2.9576e-01, -6.8816e-01, -5.9565e-01, -2.6397e-01,\n",
       "         -4.9731e-01, -1.4363e-01, -4.6121e-01, -4.9600e-01, -6.9411e-01,\n",
       "         -5.3715e-01, -1.2879e-02, -1.2450e-01,  6.3713e-01,  4.6495e-01,\n",
       "          2.1782e-01, -4.6260e-01, -1.3102e+00, -1.6793e-01, -1.4850e-01,\n",
       "         -1.5148e-01,  1.5185e-01, -4.4750e-01, -6.2129e-01,  4.1416e-01,\n",
       "         -1.1333e-01,  4.5312e-02, -7.2497e-01, -2.8796e-01,  1.8550e-01,\n",
       "          8.3676e-02,  2.1354e-01,  5.6836e-01,  3.0337e-01, -5.5085e-01,\n",
       "          3.1584e-01,  2.0492e-02, -1.2421e+00, -3.5106e-01, -4.0867e-02,\n",
       "          2.6847e-01, -1.3895e-01,  7.4631e-01, -5.4158e-01,  5.3413e-01,\n",
       "          2.2502e-01, -1.0565e+00,  2.3949e-01, -2.6133e-01, -5.0124e-02,\n",
       "         -7.0337e-01,  8.7961e-01,  9.3203e-01, -1.5549e-01,  9.1310e-02,\n",
       "          6.4579e-01, -3.6380e-01, -3.1151e-02, -4.4929e-02, -2.7730e-01,\n",
       "         -5.7355e-01, -4.3501e-02,  2.9125e-01, -1.2632e-01,  2.9414e-01,\n",
       "          5.1013e-01,  8.9000e-01, -1.2108e-01, -7.0120e-01, -1.7436e-01,\n",
       "          6.9537e-01, -8.3335e-02, -1.8049e-01,  8.0721e-02,  7.6826e-01,\n",
       "          7.9913e-01,  9.4110e-02, -6.4396e-02, -1.5075e-01,  1.4528e+00,\n",
       "         -1.0800e+00, -1.0768e+00,  5.0173e-01, -6.3724e-01,  1.0318e+00,\n",
       "          1.1829e-01,  1.1029e+00,  7.8148e-01,  9.5300e-01,  6.1096e-01,\n",
       "          3.1442e-01,  2.6489e-01, -8.4242e-02,  8.5640e-02,  6.9976e-01,\n",
       "          2.6501e-01, -4.1533e-01, -8.3485e-01, -3.8598e-02, -6.7281e-02,\n",
       "         -8.0036e-01,  3.1528e-01, -4.8691e-01, -7.5089e-01,  8.1574e-01,\n",
       "          2.6000e-01, -1.2308e+00, -7.8971e-02, -4.0305e-01,  6.4624e-01,\n",
       "         -1.8212e-01, -2.1910e-01, -7.5262e-01, -6.3778e-02, -4.4516e-01,\n",
       "          4.5421e-01,  4.6152e-01,  2.0484e-01,  2.9124e-01, -9.0760e-01,\n",
       "          1.2517e-01,  2.2524e-02, -1.5614e-01, -2.8605e-02, -1.3989e-01,\n",
       "          6.3793e-01, -4.2350e-01,  9.3440e-01,  7.4779e-01, -4.2187e-01,\n",
       "         -2.2314e-01, -5.6943e-01,  6.1364e-01, -5.1921e-01, -8.6258e-01,\n",
       "         -7.8533e-01, -2.8629e-01, -7.1256e-01,  6.5915e-01, -6.6480e-01,\n",
       "         -1.6483e-01,  2.0871e-02, -9.9745e-01,  7.9085e-01, -5.8221e-02,\n",
       "          2.8667e-01, -1.0270e-01,  2.2154e-01,  7.4553e-01,  8.1393e-01,\n",
       "         -1.1356e+00,  1.0224e+00,  9.9665e-02,  3.9616e-01,  4.5893e-01,\n",
       "          2.6867e-01, -3.9500e-01,  3.1548e-01, -1.3436e-01, -1.3968e-01,\n",
       "         -2.8366e-01,  4.3107e-01, -4.3495e-01,  2.0852e-01, -2.5250e-01,\n",
       "          4.0484e-01,  3.3188e-01, -3.7477e-01, -5.1613e-01, -7.4445e-01,\n",
       "          2.4291e-01, -7.3735e-01, -4.3314e-01, -2.6875e-01, -7.5845e-01,\n",
       "          5.9969e-01, -6.1343e-02,  1.2476e-02,  9.6668e-02,  1.5133e-01,\n",
       "         -4.6515e-01,  5.0606e-01,  9.8309e-01, -6.5590e-01, -4.6375e-02,\n",
       "         -2.4247e-01, -5.6563e-01, -3.6405e-01, -6.9911e-02, -1.4874e-02,\n",
       "          6.2135e-01, -9.8605e-01, -5.0598e-01,  4.8020e-02,  6.0526e-01,\n",
       "         -7.3267e-01, -3.1912e-01, -3.5028e-01,  1.1605e-01,  4.2521e-01,\n",
       "          1.2651e-01, -8.0736e-01, -2.1998e-01,  3.1049e-01,  1.7782e-01,\n",
       "          3.6377e-01, -6.1977e-01,  2.9136e-01,  3.5058e-01, -5.5194e-01,\n",
       "          1.9005e-01,  7.0341e-01, -4.5162e-01,  2.9731e-02,  2.1632e-02,\n",
       "          2.5743e-01,  2.7663e-01,  1.0904e-02,  6.4757e-01,  4.9086e-01,\n",
       "         -3.2960e-01, -2.7633e-01,  5.7114e-01, -4.2714e-01,  9.2262e-01,\n",
       "          5.5354e-02, -5.5984e-01, -1.2732e+00,  3.1300e-01, -4.2791e-01,\n",
       "          1.7385e-01, -2.7229e-01,  4.3423e-02,  2.0324e-01,  1.3840e+00,\n",
       "          2.4939e-01,  6.9377e-01, -4.2798e-01,  3.6598e-01, -1.3461e+00,\n",
       "          1.8631e-01, -1.0161e+00, -7.4426e-01,  2.8409e-02, -2.0424e-01,\n",
       "          2.6430e-02,  6.1049e-01, -6.9970e-03,  2.7876e-01,  9.1615e-01,\n",
       "          1.2270e+00,  2.0323e-01,  5.7970e-01,  5.4379e-01,  9.1639e-01,\n",
       "          5.2813e-01, -6.4057e-02,  5.1435e-01,  6.4515e-01, -3.1664e-01,\n",
       "          8.5300e-01,  1.0638e+00,  2.6289e-01,  8.8417e-01,  2.0036e-01,\n",
       "          1.4030e+00, -2.8129e-01,  5.2266e-01,  4.4262e-03, -5.8444e-01,\n",
       "         -1.8846e-01,  2.5286e-01, -5.1015e-01,  6.8060e-01,  2.6696e-02,\n",
       "         -2.3291e-01, -5.2454e-01,  8.1822e-01,  1.2940e-01, -4.3651e-01,\n",
       "          4.2895e-01, -4.4247e-02, -7.9403e-01,  4.2583e-02,  2.4541e-01,\n",
       "         -8.0955e-01, -8.1733e-01,  6.9523e-01,  1.8461e-01, -3.9014e-01,\n",
       "         -4.1130e-01, -7.3274e-01,  2.6100e-02,  6.2963e-02, -2.5507e-02,\n",
       "          3.8707e-01, -1.4076e-01, -2.2461e-01, -7.8671e-03,  9.7026e-01,\n",
       "         -1.8078e+00, -9.4983e-01,  1.0881e-01, -8.7789e-01, -4.1177e-01,\n",
       "         -1.1990e-01,  5.0857e-01,  1.1002e+00, -1.3541e-01,  1.9131e-01,\n",
       "         -4.0185e-01,  2.5938e-01,  4.2494e-01, -2.8207e-01,  1.9473e-01,\n",
       "          4.9185e-01,  2.2309e-01,  4.9925e-01,  3.4166e-01, -5.8438e-02,\n",
       "          9.8023e-02, -1.5041e-01, -5.2703e-03,  7.7499e-02, -8.1372e-03,\n",
       "         -5.7876e-01, -2.1627e-01, -2.9497e-01,  1.1781e+00,  9.0086e-02,\n",
       "         -2.9813e-01, -3.1783e-01, -3.9352e-01, -8.4078e-01, -2.3041e-01,\n",
       "         -3.6594e-01, -5.0107e-02, -2.4397e-01,  5.5381e-02,  1.3256e-01,\n",
       "         -9.7787e-01, -4.5854e-01, -1.6199e-01,  1.5669e-01,  6.3530e-01,\n",
       "          7.2060e-02,  7.0581e-01, -1.5763e-01,  5.2845e-01,  1.1442e+00,\n",
       "         -1.8868e-01,  7.1706e-01, -3.8935e-01,  2.0357e-01, -4.5133e-01,\n",
       "          1.2269e+00,  5.5485e-02, -4.7278e-01, -8.0677e-01, -1.8590e-01,\n",
       "         -2.7514e-03, -1.2598e+00, -2.1265e-01,  2.7330e-01, -4.7164e-02,\n",
       "         -2.6165e-01, -3.1191e-01, -1.2583e+00, -1.6705e-01, -1.4262e-01,\n",
       "         -7.6561e-02, -3.3681e-01,  5.7711e-01, -4.4000e-01,  2.2498e-01,\n",
       "         -7.7210e-01, -5.9465e-01, -1.1508e-01,  3.0691e-01, -8.5919e-02,\n",
       "         -3.7216e-02,  9.8057e-01,  1.6305e-01,  5.1025e-01,  3.8774e-01,\n",
       "          9.7789e-02,  3.5406e-01,  2.4142e-01,  2.2113e-01, -2.7591e-01,\n",
       "          5.2307e-01, -7.1516e-01, -2.4002e-01, -2.2461e-01,  1.8580e-02,\n",
       "         -4.4363e-01, -8.1640e-01,  8.6717e-01,  6.4184e-01,  3.0016e-02,\n",
       "         -1.5161e+00, -5.6651e-02,  5.5042e-01, -1.1416e-01, -3.3988e-01,\n",
       "          4.0066e-01,  3.6259e-03,  1.2082e-01,  2.5781e-01,  3.8019e-01,\n",
       "         -2.1587e-01,  7.0850e-01, -9.5638e-01,  3.8302e-01, -3.7670e-01,\n",
       "          4.8424e-01, -3.1480e-01,  4.3842e-01, -1.4860e-02,  6.7275e-01,\n",
       "         -6.1506e-01, -4.3798e-01,  8.2841e-01,  5.0537e-01,  1.5964e-02,\n",
       "         -5.0280e-02,  2.5556e-02,  9.7351e-01, -5.2554e-01,  6.4182e-01,\n",
       "          3.7221e-01, -5.1202e-01, -3.6115e-01, -8.0215e-01,  4.4038e-01,\n",
       "          6.2495e-01, -1.4039e-01,  9.7562e-01, -7.3043e-01, -1.4216e+00,\n",
       "          2.6512e-01,  3.6043e-01,  3.5215e-01, -1.4793e-03, -3.9073e-01,\n",
       "          9.4557e-02, -5.1305e-01, -5.5081e-01, -2.7893e-01,  1.0279e+00,\n",
       "         -1.9544e-01, -1.0516e+00, -1.2739e-02,  1.4274e-01, -2.2467e-01,\n",
       "         -6.1238e-01, -4.9684e-01,  3.7468e-01,  2.6355e-01, -1.3723e-01,\n",
       "          2.3372e-01, -4.5239e-01,  3.7555e-01, -1.5366e-01, -9.4159e-01,\n",
       "         -6.8213e-01, -8.6040e-02,  3.4298e-01, -3.3628e-01,  4.0141e-01,\n",
       "         -5.3914e-01,  1.2381e+00, -7.4657e-01,  3.8510e-01,  3.7425e-01,\n",
       "          1.2679e-01, -4.3848e-01, -4.8026e-01, -5.3142e-01,  1.2723e-01,\n",
       "         -8.6732e-02,  2.3175e-01,  5.6923e-01, -4.7455e-01,  1.1628e-01,\n",
       "          1.6017e-01, -1.7760e-01,  3.6175e-01,  3.4515e-01, -2.4161e-01,\n",
       "         -6.1984e-01,  4.6961e-01, -8.2918e-01, -3.7765e-02, -5.9844e-01,\n",
       "          4.9794e-01, -8.0569e-01,  3.5460e-01, -6.6633e-01,  6.2138e-01,\n",
       "          8.7946e-01,  5.2704e-01,  6.0699e-01, -3.7001e-01,  5.6264e-01,\n",
       "          8.6536e-01, -5.7895e-01, -1.0739e-01,  2.6680e-01, -8.9108e-01,\n",
       "         -3.7710e-01, -2.2461e-01, -3.6557e-01, -2.8231e-01,  5.7895e-01,\n",
       "          3.1546e-03,  6.5128e-01,  1.9697e-02, -4.0349e-01,  6.0735e-01,\n",
       "          1.6060e-01,  1.2778e+00, -1.7686e-01, -1.2261e+00, -4.2551e-01,\n",
       "         -3.4150e-01,  9.2878e-01, -7.1890e-02,  2.2121e-01, -3.1407e-01,\n",
       "         -1.4557e-01,  4.3569e-01, -5.2289e-01, -8.5221e-01,  1.7036e-01,\n",
       "          3.7086e-01,  4.8385e-02,  9.1072e-01, -3.2017e-01,  5.3161e-03,\n",
       "         -2.3468e-01, -3.6661e-01, -2.3791e-01, -6.4114e-01,  6.4166e-01,\n",
       "          6.9287e-01,  1.7073e-01,  2.6416e-03,  5.5939e-02, -4.7718e-01,\n",
       "         -1.0314e+00,  4.1633e-01, -5.3653e-01, -3.3433e-01, -7.0561e-01,\n",
       "         -1.1301e-01,  9.3087e-02, -1.9829e-01,  7.6303e-01,  3.3358e-02,\n",
       "         -1.2512e-01,  7.0877e-01, -5.0847e-02,  4.9821e-01, -3.9941e-02,\n",
       "         -2.7969e-01,  1.4614e+00, -4.5707e-03,  1.6425e-01,  2.0109e-01,\n",
       "         -4.0344e-01,  1.9424e-01, -3.2237e-02, -4.9517e-01, -6.8365e-01,\n",
       "          8.7437e-01,  2.6613e-01, -6.5661e-01, -6.4488e-02, -4.3823e-01,\n",
       "         -2.5440e-01,  1.1278e+00,  6.0338e-03, -3.2592e-01,  8.8422e-01,\n",
       "          3.4187e-01, -3.0148e-01, -3.5588e-01,  5.1285e-01,  2.3557e-01,\n",
       "          3.7929e-01, -5.5789e-01,  8.5455e-01, -6.4634e-01, -3.4453e-01,\n",
       "          5.2766e-01, -1.7449e-01,  6.9555e-01,  1.7740e-01, -3.6719e-01,\n",
       "          6.7172e-02, -1.5985e-01,  9.6661e-01,  1.3317e+00,  6.0235e-01,\n",
       "          5.6350e-01, -2.6367e-01,  6.5282e-01, -6.8270e-01,  4.0452e-01,\n",
       "         -3.9469e-01, -9.5936e-02,  5.3953e-01, -3.0780e-01,  9.2171e-02,\n",
       "         -7.9259e-01, -7.1034e-01, -5.8743e-01,  1.1540e+00, -1.7577e-01,\n",
       "         -1.0312e+00, -8.8544e-03,  2.0130e-01,  1.1107e+00,  3.8460e-01,\n",
       "          4.4353e-01, -2.4764e-01, -6.3883e-01,  7.1505e-01, -9.6007e-01,\n",
       "         -7.6634e-01, -5.5998e-01, -1.0079e-01,  8.9775e-01, -7.0632e-01,\n",
       "          8.8160e-01,  6.7121e-01,  2.7866e-01,  1.7980e-01,  1.2418e-01,\n",
       "         -5.4339e-01,  2.7600e-01, -8.6587e-01, -1.9773e-01,  1.9971e-01,\n",
       "         -9.6920e-01, -7.9926e-01,  5.4796e-02, -1.2470e-01, -3.0378e-01,\n",
       "          3.1086e-01,  5.9532e-02,  8.5259e-02, -2.9468e-01, -1.3709e-01,\n",
       "         -3.9983e-02, -5.2465e-01,  2.1986e-02, -4.8507e-01, -5.9446e-01,\n",
       "          3.0063e-01,  6.8747e-01,  5.1973e-01, -3.0225e-01,  3.8694e-01,\n",
       "         -4.4809e-02, -2.1334e-01, -4.2080e-01, -1.0693e+00,  8.1038e-02,\n",
       "          1.1675e-01,  2.9618e-01, -1.2956e-01,  5.9236e-01,  4.3762e-01,\n",
       "          6.1130e-01,  7.3930e-01,  4.2660e-01, -6.2401e-01, -3.0565e-01,\n",
       "          1.8072e-01,  4.0078e-01,  4.0623e-01, -1.6712e-01, -2.3685e-01,\n",
       "          9.8010e-01, -1.2945e-01, -5.3189e-01,  7.0496e-01, -4.3770e-02,\n",
       "          1.9512e-01,  3.7857e-01,  9.8297e-01, -2.5091e-01,  4.7186e-01,\n",
       "         -9.3318e-01,  6.8205e-02,  2.4255e-01,  2.7788e-01,  6.4255e-01,\n",
       "         -2.3546e-01,  9.9033e-01,  8.8483e-01,  2.1334e-01, -2.6152e-01,\n",
       "          2.4146e-01,  8.5763e-01,  6.6597e-02,  8.8662e-01,  5.1994e-01,\n",
       "          4.2172e-01, -2.9435e-01, -6.3574e-01, -7.1130e-01,  7.6055e-01,\n",
       "         -7.0141e-01, -4.9442e-01,  2.2450e-01,  1.5947e-01,  4.1330e-01,\n",
       "         -2.2326e-01, -9.2154e-01, -2.9033e-01,  2.0797e-01, -7.4245e-01,\n",
       "         -5.3978e-02,  2.8450e-01,  2.3549e-01,  7.9992e-02,  4.6440e-02,\n",
       "         -3.3617e-01, -9.4231e-01, -7.9664e-01,  7.3817e-01,  8.4103e-01,\n",
       "          1.4210e-02, -7.4851e-01, -5.1323e-01, -6.1785e-01,  1.0618e-01,\n",
       "         -4.7821e-01, -1.1305e+00,  1.4729e-01, -9.2180e-02,  1.2113e+00,\n",
       "         -1.3004e+00, -6.5109e-01, -8.2105e-01, -4.3249e-01,  5.0520e-01,\n",
       "          4.3780e-01,  8.6309e-01, -1.0217e-01, -4.3633e-01,  4.3351e-01,\n",
       "         -1.5336e-01, -1.8384e-01, -6.5583e-01, -3.1923e-01, -7.4245e-02,\n",
       "          1.4555e-01, -1.1131e+00,  5.1829e-01, -2.0940e-01, -8.5685e-01,\n",
       "         -1.9115e-01, -6.5065e-01,  7.1851e-01,  7.6865e-01,  2.5364e-01,\n",
       "         -3.3996e-01, -5.0115e-01, -5.3989e-01,  1.0059e+00, -1.1280e+00,\n",
       "          8.9841e-02,  5.6907e-01,  2.5438e-01, -4.5735e-01,  1.0364e+00,\n",
       "         -7.8055e-02, -1.5194e-01,  1.4930e+00, -2.6903e-01,  2.1734e-01,\n",
       "          1.4477e+00,  2.1406e-01, -8.9784e-01,  2.6629e-01,  6.6988e-01,\n",
       "          4.0469e-01, -2.6916e-01,  9.4931e-01,  3.9139e-01,  1.2255e-01,\n",
       "          3.0457e-01,  4.7288e-01,  1.3430e-01,  4.4670e-01,  8.3956e-01,\n",
       "          1.0234e+00, -1.0775e+00,  7.5444e-01, -8.0895e-02,  6.1299e-01,\n",
       "         -7.1358e-02,  2.6708e-01,  4.8964e-01,  2.8376e-01, -1.8979e-01,\n",
       "          3.2918e-01, -8.1109e-01, -5.3686e-01, -3.1404e-01,  3.0646e-01,\n",
       "          3.8707e-01,  1.7045e-01,  1.2823e-01, -3.7656e-02,  3.6558e-04,\n",
       "          2.3967e-01, -3.6833e-01,  4.8848e-01,  3.0198e-01,  4.1311e-01,\n",
       "         -2.8299e-01, -7.6845e-01, -2.3205e-01,  9.2272e-01,  1.2358e+00,\n",
       "         -4.7025e-01,  2.5476e-01, -8.3458e-01, -1.8636e-01,  9.0231e-01,\n",
       "          7.5564e-01, -1.3180e-01,  7.2439e-01,  5.1036e-01, -1.4946e+00,\n",
       "         -8.4151e-01,  1.1249e+00,  4.5903e-01, -4.1825e-01, -6.4708e-01,\n",
       "          9.3160e-02, -1.3134e-01,  2.1776e-01, -6.8011e-01, -8.3280e-01,\n",
       "         -1.1620e-01, -1.6584e-01, -8.7894e-01,  2.1643e-01,  3.0562e-01,\n",
       "          9.1447e-02, -3.8287e-01,  8.3501e-02, -1.6328e-01, -1.0556e-01,\n",
       "         -5.4905e-01,  2.4004e-01,  3.7425e-01, -2.2720e-01, -2.6135e-01,\n",
       "          6.7519e-01, -1.6433e-01, -6.8688e-01,  3.7427e-01,  1.0410e-01,\n",
       "         -1.2674e-01, -3.5135e-01,  4.6717e-01,  1.0264e-01,  1.3437e-01,\n",
       "          1.7437e-01,  4.1220e-01, -4.7952e-01,  1.5758e+00,  5.9172e-01,\n",
       "          4.8109e-01, -2.4540e-01, -5.1269e-01, -1.9937e-01,  1.6227e-01,\n",
       "          4.4511e-01, -6.5701e-01,  3.5444e-01, -3.7614e-01, -1.8614e-01,\n",
       "         -3.4692e-01,  2.4885e-01, -6.4295e-01,  3.8428e-01, -6.0093e-01,\n",
       "         -3.4472e-01, -6.3437e-01, -6.7428e-01,  5.8320e-01, -1.7674e-02,\n",
       "         -1.7488e-01,  3.4488e-01,  4.7314e-01, -2.5481e-01,  1.0236e+00,\n",
       "         -1.4951e+00,  2.2308e-01,  4.6357e-01,  9.6061e-01,  2.9151e-01,\n",
       "          8.7914e-01,  7.1921e-01,  4.7794e-01, -1.1136e+00, -3.2337e-01,\n",
       "         -1.3616e+00,  3.6235e-01,  4.9819e-01, -3.9247e-01, -3.0211e-01,\n",
       "         -2.2252e-01, -1.4764e-01, -1.8636e-01,  1.3039e-01,  4.8808e-01,\n",
       "         -6.9631e-01,  2.4601e-01,  1.8667e-01,  1.9178e-01, -1.1680e-01,\n",
       "          4.6233e-01, -6.9137e-01,  2.1696e-01,  5.8124e-02, -9.0744e-01,\n",
       "         -3.9777e-01, -7.9183e-02,  2.7839e-01, -5.1523e-01,  5.5712e-01,\n",
       "          5.9418e-01, -7.0301e-01,  8.6508e-01, -1.0519e-02, -4.0363e-01,\n",
       "         -3.9494e-01, -6.9064e-01,  2.8317e-01,  1.8656e-01, -1.5910e-01,\n",
       "          4.7721e-01,  6.5027e-01, -1.1444e-02, -5.2837e-01, -1.1521e+00,\n",
       "          6.4925e-01,  2.3718e-01, -2.3399e-01,  1.1608e-01, -1.2512e+00]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swin_transformer(dummy_input) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/aws_neuron_venv_pytorch/lib/python3.8/site-packages/torch/jit/_trace.py:1001: TracerWarning: Output nr 1. of the traced function does not match the corresponding output of the Python function. Detailed error:\n",
      "Tensor-likes are not close!\n",
      "\n",
      "Mismatched elements: 1000 / 1000 (100.0%)\n",
      "Greatest absolute difference: 1.0012190006673336 at index (0, 409) (up to 1e-05 allowed)\n",
      "Greatest relative difference: 1479.7131770592641 at index (0, 529) (up to 1e-05 allowed)\n",
      "  _check_trace(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SwinTransformer(\n",
       "  original_name=SwinTransformer\n",
       "  (patch_embed): PatchEmbed(\n",
       "    original_name=PatchEmbed\n",
       "    (proj): Conv2d(original_name=Conv2d)\n",
       "    (norm): LayerNorm(original_name=LayerNorm)\n",
       "  )\n",
       "  (pos_drop): Dropout(original_name=Dropout)\n",
       "  (layers): Sequential(\n",
       "    original_name=Sequential\n",
       "    (0): BasicLayer(\n",
       "      original_name=BasicLayer\n",
       "      (blocks): ModuleList(\n",
       "        original_name=ModuleList\n",
       "        (0): SwinTransformerBlock(\n",
       "          original_name=SwinTransformerBlock\n",
       "          (norm1): LayerNorm(original_name=LayerNorm)\n",
       "          (attn): WindowAttention(\n",
       "            original_name=WindowAttention\n",
       "            (qkv): Linear(original_name=Linear)\n",
       "            (attn_drop): Dropout(original_name=Dropout)\n",
       "            (proj): Linear(original_name=Linear)\n",
       "            (proj_drop): Dropout(original_name=Dropout)\n",
       "            (softmax): Softmax(original_name=Softmax)\n",
       "          )\n",
       "          (drop_path): Identity(original_name=Identity)\n",
       "          (norm2): LayerNorm(original_name=LayerNorm)\n",
       "          (mlp): Mlp(\n",
       "            original_name=Mlp\n",
       "            (fc1): Linear(original_name=Linear)\n",
       "            (act): GELU(original_name=GELU)\n",
       "            (drop1): Dropout(original_name=Dropout)\n",
       "            (fc2): Linear(original_name=Linear)\n",
       "            (drop2): Dropout(original_name=Dropout)\n",
       "          )\n",
       "        )\n",
       "        (1): SwinTransformerBlock(\n",
       "          original_name=SwinTransformerBlock\n",
       "          (norm1): LayerNorm(original_name=LayerNorm)\n",
       "          (attn): WindowAttention(\n",
       "            original_name=WindowAttention\n",
       "            (qkv): Linear(original_name=Linear)\n",
       "            (attn_drop): Dropout(original_name=Dropout)\n",
       "            (proj): Linear(original_name=Linear)\n",
       "            (proj_drop): Dropout(original_name=Dropout)\n",
       "            (softmax): Softmax(original_name=Softmax)\n",
       "          )\n",
       "          (drop_path): DropPath(original_name=DropPath)\n",
       "          (norm2): LayerNorm(original_name=LayerNorm)\n",
       "          (mlp): Mlp(\n",
       "            original_name=Mlp\n",
       "            (fc1): Linear(original_name=Linear)\n",
       "            (act): GELU(original_name=GELU)\n",
       "            (drop1): Dropout(original_name=Dropout)\n",
       "            (fc2): Linear(original_name=Linear)\n",
       "            (drop2): Dropout(original_name=Dropout)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (downsample): PatchMerging(\n",
       "        original_name=PatchMerging\n",
       "        (reduction): Linear(original_name=Linear)\n",
       "        (norm): LayerNorm(original_name=LayerNorm)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicLayer(\n",
       "      original_name=BasicLayer\n",
       "      (blocks): ModuleList(\n",
       "        original_name=ModuleList\n",
       "        (0): SwinTransformerBlock(\n",
       "          original_name=SwinTransformerBlock\n",
       "          (norm1): LayerNorm(original_name=LayerNorm)\n",
       "          (attn): WindowAttention(\n",
       "            original_name=WindowAttention\n",
       "            (qkv): Linear(original_name=Linear)\n",
       "            (attn_drop): Dropout(original_name=Dropout)\n",
       "            (proj): Linear(original_name=Linear)\n",
       "            (proj_drop): Dropout(original_name=Dropout)\n",
       "            (softmax): Softmax(original_name=Softmax)\n",
       "          )\n",
       "          (drop_path): DropPath(original_name=DropPath)\n",
       "          (norm2): LayerNorm(original_name=LayerNorm)\n",
       "          (mlp): Mlp(\n",
       "            original_name=Mlp\n",
       "            (fc1): Linear(original_name=Linear)\n",
       "            (act): GELU(original_name=GELU)\n",
       "            (drop1): Dropout(original_name=Dropout)\n",
       "            (fc2): Linear(original_name=Linear)\n",
       "            (drop2): Dropout(original_name=Dropout)\n",
       "          )\n",
       "        )\n",
       "        (1): SwinTransformerBlock(\n",
       "          original_name=SwinTransformerBlock\n",
       "          (norm1): LayerNorm(original_name=LayerNorm)\n",
       "          (attn): WindowAttention(\n",
       "            original_name=WindowAttention\n",
       "            (qkv): Linear(original_name=Linear)\n",
       "            (attn_drop): Dropout(original_name=Dropout)\n",
       "            (proj): Linear(original_name=Linear)\n",
       "            (proj_drop): Dropout(original_name=Dropout)\n",
       "            (softmax): Softmax(original_name=Softmax)\n",
       "          )\n",
       "          (drop_path): DropPath(original_name=DropPath)\n",
       "          (norm2): LayerNorm(original_name=LayerNorm)\n",
       "          (mlp): Mlp(\n",
       "            original_name=Mlp\n",
       "            (fc1): Linear(original_name=Linear)\n",
       "            (act): GELU(original_name=GELU)\n",
       "            (drop1): Dropout(original_name=Dropout)\n",
       "            (fc2): Linear(original_name=Linear)\n",
       "            (drop2): Dropout(original_name=Dropout)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (downsample): PatchMerging(\n",
       "        original_name=PatchMerging\n",
       "        (reduction): Linear(original_name=Linear)\n",
       "        (norm): LayerNorm(original_name=LayerNorm)\n",
       "      )\n",
       "    )\n",
       "    (2): BasicLayer(\n",
       "      original_name=BasicLayer\n",
       "      (blocks): ModuleList(\n",
       "        original_name=ModuleList\n",
       "        (0): SwinTransformerBlock(\n",
       "          original_name=SwinTransformerBlock\n",
       "          (norm1): LayerNorm(original_name=LayerNorm)\n",
       "          (attn): WindowAttention(\n",
       "            original_name=WindowAttention\n",
       "            (qkv): Linear(original_name=Linear)\n",
       "            (attn_drop): Dropout(original_name=Dropout)\n",
       "            (proj): Linear(original_name=Linear)\n",
       "            (proj_drop): Dropout(original_name=Dropout)\n",
       "            (softmax): Softmax(original_name=Softmax)\n",
       "          )\n",
       "          (drop_path): DropPath(original_name=DropPath)\n",
       "          (norm2): LayerNorm(original_name=LayerNorm)\n",
       "          (mlp): Mlp(\n",
       "            original_name=Mlp\n",
       "            (fc1): Linear(original_name=Linear)\n",
       "            (act): GELU(original_name=GELU)\n",
       "            (drop1): Dropout(original_name=Dropout)\n",
       "            (fc2): Linear(original_name=Linear)\n",
       "            (drop2): Dropout(original_name=Dropout)\n",
       "          )\n",
       "        )\n",
       "        (1): SwinTransformerBlock(\n",
       "          original_name=SwinTransformerBlock\n",
       "          (norm1): LayerNorm(original_name=LayerNorm)\n",
       "          (attn): WindowAttention(\n",
       "            original_name=WindowAttention\n",
       "            (qkv): Linear(original_name=Linear)\n",
       "            (attn_drop): Dropout(original_name=Dropout)\n",
       "            (proj): Linear(original_name=Linear)\n",
       "            (proj_drop): Dropout(original_name=Dropout)\n",
       "            (softmax): Softmax(original_name=Softmax)\n",
       "          )\n",
       "          (drop_path): DropPath(original_name=DropPath)\n",
       "          (norm2): LayerNorm(original_name=LayerNorm)\n",
       "          (mlp): Mlp(\n",
       "            original_name=Mlp\n",
       "            (fc1): Linear(original_name=Linear)\n",
       "            (act): GELU(original_name=GELU)\n",
       "            (drop1): Dropout(original_name=Dropout)\n",
       "            (fc2): Linear(original_name=Linear)\n",
       "            (drop2): Dropout(original_name=Dropout)\n",
       "          )\n",
       "        )\n",
       "        (2): SwinTransformerBlock(\n",
       "          original_name=SwinTransformerBlock\n",
       "          (norm1): LayerNorm(original_name=LayerNorm)\n",
       "          (attn): WindowAttention(\n",
       "            original_name=WindowAttention\n",
       "            (qkv): Linear(original_name=Linear)\n",
       "            (attn_drop): Dropout(original_name=Dropout)\n",
       "            (proj): Linear(original_name=Linear)\n",
       "            (proj_drop): Dropout(original_name=Dropout)\n",
       "            (softmax): Softmax(original_name=Softmax)\n",
       "          )\n",
       "          (drop_path): DropPath(original_name=DropPath)\n",
       "          (norm2): LayerNorm(original_name=LayerNorm)\n",
       "          (mlp): Mlp(\n",
       "            original_name=Mlp\n",
       "            (fc1): Linear(original_name=Linear)\n",
       "            (act): GELU(original_name=GELU)\n",
       "            (drop1): Dropout(original_name=Dropout)\n",
       "            (fc2): Linear(original_name=Linear)\n",
       "            (drop2): Dropout(original_name=Dropout)\n",
       "          )\n",
       "        )\n",
       "        (3): SwinTransformerBlock(\n",
       "          original_name=SwinTransformerBlock\n",
       "          (norm1): LayerNorm(original_name=LayerNorm)\n",
       "          (attn): WindowAttention(\n",
       "            original_name=WindowAttention\n",
       "            (qkv): Linear(original_name=Linear)\n",
       "            (attn_drop): Dropout(original_name=Dropout)\n",
       "            (proj): Linear(original_name=Linear)\n",
       "            (proj_drop): Dropout(original_name=Dropout)\n",
       "            (softmax): Softmax(original_name=Softmax)\n",
       "          )\n",
       "          (drop_path): DropPath(original_name=DropPath)\n",
       "          (norm2): LayerNorm(original_name=LayerNorm)\n",
       "          (mlp): Mlp(\n",
       "            original_name=Mlp\n",
       "            (fc1): Linear(original_name=Linear)\n",
       "            (act): GELU(original_name=GELU)\n",
       "            (drop1): Dropout(original_name=Dropout)\n",
       "            (fc2): Linear(original_name=Linear)\n",
       "            (drop2): Dropout(original_name=Dropout)\n",
       "          )\n",
       "        )\n",
       "        (4): SwinTransformerBlock(\n",
       "          original_name=SwinTransformerBlock\n",
       "          (norm1): LayerNorm(original_name=LayerNorm)\n",
       "          (attn): WindowAttention(\n",
       "            original_name=WindowAttention\n",
       "            (qkv): Linear(original_name=Linear)\n",
       "            (attn_drop): Dropout(original_name=Dropout)\n",
       "            (proj): Linear(original_name=Linear)\n",
       "            (proj_drop): Dropout(original_name=Dropout)\n",
       "            (softmax): Softmax(original_name=Softmax)\n",
       "          )\n",
       "          (drop_path): DropPath(original_name=DropPath)\n",
       "          (norm2): LayerNorm(original_name=LayerNorm)\n",
       "          (mlp): Mlp(\n",
       "            original_name=Mlp\n",
       "            (fc1): Linear(original_name=Linear)\n",
       "            (act): GELU(original_name=GELU)\n",
       "            (drop1): Dropout(original_name=Dropout)\n",
       "            (fc2): Linear(original_name=Linear)\n",
       "            (drop2): Dropout(original_name=Dropout)\n",
       "          )\n",
       "        )\n",
       "        (5): SwinTransformerBlock(\n",
       "          original_name=SwinTransformerBlock\n",
       "          (norm1): LayerNorm(original_name=LayerNorm)\n",
       "          (attn): WindowAttention(\n",
       "            original_name=WindowAttention\n",
       "            (qkv): Linear(original_name=Linear)\n",
       "            (attn_drop): Dropout(original_name=Dropout)\n",
       "            (proj): Linear(original_name=Linear)\n",
       "            (proj_drop): Dropout(original_name=Dropout)\n",
       "            (softmax): Softmax(original_name=Softmax)\n",
       "          )\n",
       "          (drop_path): DropPath(original_name=DropPath)\n",
       "          (norm2): LayerNorm(original_name=LayerNorm)\n",
       "          (mlp): Mlp(\n",
       "            original_name=Mlp\n",
       "            (fc1): Linear(original_name=Linear)\n",
       "            (act): GELU(original_name=GELU)\n",
       "            (drop1): Dropout(original_name=Dropout)\n",
       "            (fc2): Linear(original_name=Linear)\n",
       "            (drop2): Dropout(original_name=Dropout)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (downsample): PatchMerging(\n",
       "        original_name=PatchMerging\n",
       "        (reduction): Linear(original_name=Linear)\n",
       "        (norm): LayerNorm(original_name=LayerNorm)\n",
       "      )\n",
       "    )\n",
       "    (3): BasicLayer(\n",
       "      original_name=BasicLayer\n",
       "      (blocks): ModuleList(\n",
       "        original_name=ModuleList\n",
       "        (0): SwinTransformerBlock(\n",
       "          original_name=SwinTransformerBlock\n",
       "          (norm1): LayerNorm(original_name=LayerNorm)\n",
       "          (attn): WindowAttention(\n",
       "            original_name=WindowAttention\n",
       "            (qkv): Linear(original_name=Linear)\n",
       "            (attn_drop): Dropout(original_name=Dropout)\n",
       "            (proj): Linear(original_name=Linear)\n",
       "            (proj_drop): Dropout(original_name=Dropout)\n",
       "            (softmax): Softmax(original_name=Softmax)\n",
       "          )\n",
       "          (drop_path): DropPath(original_name=DropPath)\n",
       "          (norm2): LayerNorm(original_name=LayerNorm)\n",
       "          (mlp): Mlp(\n",
       "            original_name=Mlp\n",
       "            (fc1): Linear(original_name=Linear)\n",
       "            (act): GELU(original_name=GELU)\n",
       "            (drop1): Dropout(original_name=Dropout)\n",
       "            (fc2): Linear(original_name=Linear)\n",
       "            (drop2): Dropout(original_name=Dropout)\n",
       "          )\n",
       "        )\n",
       "        (1): SwinTransformerBlock(\n",
       "          original_name=SwinTransformerBlock\n",
       "          (norm1): LayerNorm(original_name=LayerNorm)\n",
       "          (attn): WindowAttention(\n",
       "            original_name=WindowAttention\n",
       "            (qkv): Linear(original_name=Linear)\n",
       "            (attn_drop): Dropout(original_name=Dropout)\n",
       "            (proj): Linear(original_name=Linear)\n",
       "            (proj_drop): Dropout(original_name=Dropout)\n",
       "            (softmax): Softmax(original_name=Softmax)\n",
       "          )\n",
       "          (drop_path): DropPath(original_name=DropPath)\n",
       "          (norm2): LayerNorm(original_name=LayerNorm)\n",
       "          (mlp): Mlp(\n",
       "            original_name=Mlp\n",
       "            (fc1): Linear(original_name=Linear)\n",
       "            (act): GELU(original_name=GELU)\n",
       "            (drop1): Dropout(original_name=Dropout)\n",
       "            (fc2): Linear(original_name=Linear)\n",
       "            (drop2): Dropout(original_name=Dropout)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm(original_name=LayerNorm)\n",
       "  (avgpool): AdaptiveAvgPool1d(original_name=AdaptiveAvgPool1d)\n",
       "  (head): Linear(original_name=Linear)\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.jit.trace(swin_transformer, dummy_input) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "swin_transformer_neuron = torch_neuronx.trace(swin_transformer, dummy_input) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-02T12:53:27Z WARNING 42985 [StaticProfiler]: matmul-based transposes inserted by penguin takes up 90.94 percent of all matmul computation\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NeuronModule(original_name=NeuronModule)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# When BasicLayer's depth is 1\n",
    "from timm.models.swin_transformer import BasicLayer\n",
    "\n",
    "params = {\n",
    "    \"dim\": 16,\n",
    "    #\"out_dim\": 32,\n",
    "    \"input_resolution\": (80, 240),\n",
    "    \"depth\": 1,\n",
    "    \"num_heads\": 4,\n",
    "    #\"head_dim\": None,\n",
    "    \"window_size\": 10,\n",
    "    \"mlp_ratio\": 4.0,\n",
    "    \"qkv_bias\": True\n",
    "}\n",
    "basic_layer = BasicLayer(**params)\n",
    "dummy_input = torch.zeros((1, params[\"input_resolution\"][0]*params[\"input_resolution\"][1], params[\"dim\"]))\n",
    "basic_layer_output = basic_layer(dummy_input)\n",
    "torch.jit.trace(basic_layer, dummy_input)\n",
    "torch_neuronx.trace(basic_layer, dummy_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-02T12:54:44Z WARNING 43331 [StaticProfiler]: matmul-based transposes inserted by penguin takes up 90.82 percent of all matmul computation\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NeuronModule(original_name=NeuronModule)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# when the depth is 2\n",
    "params[\"depth\"] = 2 # Now testing two sequentially connected BasicLayers\n",
    "basic_layer = BasicLayer(**params)\n",
    "dummy_input = torch.zeros((1, params[\"input_resolution\"][0]*params[\"input_resolution\"][1], params[\"dim\"]))\n",
    "basic_layer_output = basic_layer(dummy_input)\n",
    "torch.jit.trace(basic_layer, dummy_input)\n",
    "torch_neuronx.trace(basic_layer, dummy_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-02T12:55:58Z WARNING 43928 [StaticProfiler]: matmul-based transposes inserted by penguin takes up 87.98 percent of all matmul computation\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NeuronModule(original_name=NeuronModule)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the tracing goes right when depth = 4 (or any other number but 2)\n",
    "params[\"depth\"] = 4 # Now testing two sequentially connected BasicLayers\n",
    "basic_layer = BasicLayer(**params)\n",
    "dummy_input = torch.zeros((1, params[\"input_resolution\"][0]*params[\"input_resolution\"][1], params[\"dim\"]))\n",
    "basic_layer_output = basic_layer(dummy_input)\n",
    "torch.jit.trace(basic_layer, dummy_input)\n",
    "torch_neuronx.trace(basic_layer, dummy_input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
