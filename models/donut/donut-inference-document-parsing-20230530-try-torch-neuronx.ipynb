{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# torch-neuronx requires 1.13.1 or more above\n",
    "\n",
    "# https://github.com/clovaai/donut/issues/132#issuecomment-1412385775\n",
    "!pip install protobuf==3.19.6 --quiet\n",
    "!pip install pytorch-lightning==1.6.4 --quiet\n",
    "!pip install transformers --quiet\n",
    "!pip install torchvision --quiet\n",
    "!pip install torch==1.13.1 --quiet\n",
    "!pip install timm==0.5.4 --quiet\n",
    "!pip install donut-python==1.0.9 --quiet\n",
    "!pip install neuronx-cc==2.* torch-neuronx --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Desired=Unknown/Install/Remove/Purge/Hold\r\n",
      "| Status=Not/Inst/Conf-files/Unpacked/halF-conf/Half-inst/trig-aWait/Trig-pend\r\n",
      "|/ Err?=(none)/Reinst-required (Status,Err: uppercase=bad)\r\n",
      "||/ Name                    Version            Architecture Description\r\n",
      "+++-=======================-==================-============-=================================\r\n",
      "ii  aws-neuronx-runtime-lib 2.13.6.0-29de104d6 amd64        neuron_runtime built using CMake\r\n"
     ]
    }
   ],
   "source": [
    "!dpkg -l aws-neuronx-runtime-lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aws-neuronx-runtime-discovery 2.9\r\n",
      "donut-python                  1.0.9\r\n",
      "libneuronxla                  0.5.207\r\n",
      "neuronx-cc                    2.6.0.19+3d819e565\r\n",
      "neuronx-hwm                   2.6.0.0+826e77395\r\n",
      "pytorch-lightning             1.6.4\r\n",
      "torch                         1.13.1\r\n",
      "torch-neuronx                 1.13.1.1.7.0\r\n",
      "torch-xla                     1.13.1+torchneuron6\r\n",
      "torchmetrics                  0.11.4\r\n",
      "torchvision                   0.14.1\r\n"
     ]
    }
   ],
   "source": [
    "!pip list |grep -e torch -e neuron -e donut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/aws_neuron_venv_pytorch/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: Python 3.8.10\n",
      "\n",
      "CUDA version: 11.8\n",
      "\n",
      "torch version: 1.13.1+cu117\n",
      "\n",
      "transformers version: 4.11.3\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import subprocess\n",
    "\n",
    "# 現在の日時を取得\n",
    "now = datetime.datetime.now()\n",
    "date_str = now.strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "# Pythonバージョンの確認\n",
    "python_version = subprocess.run(['python', '--version'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout.strip()\n",
    "\n",
    "# CUDAバージョンの確認\n",
    "try:\n",
    "    nvcc_version = subprocess.run(['nvcc', '--version'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout.strip()\n",
    "    _, cuda_version = nvcc_version.split('\\n')[-2].split(',')[1].split()\n",
    "except:\n",
    "    cuda_version = 'CPU'\n",
    "\n",
    "# torchバージョンの確認\n",
    "import torch\n",
    "torch_version = torch.__version__\n",
    "\n",
    "# transformersバージョンの確認\n",
    "import transformers\n",
    "transformers_version = transformers.__version__\n",
    "\n",
    "# 実行時のコマンドと実行結果を保存するファイル名\n",
    "log_file = f\"diffusion_{date_str}.log\"\n",
    "\n",
    "\n",
    "print(f\"Python version: {python_version}\\n\")\n",
    "print(f\"CUDA version: {cuda_version}\\n\")\n",
    "print(f\"torch version: {torch_version}\\n\")\n",
    "print(f\"transformers version: {transformers_version}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import sys\n",
    "from donut import DonutModel\n",
    "\n",
    "import IPython\n",
    "import pprint\n",
    "from donut.model import DonutConfig, DonutModel, SwinEncoder, BARTDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import re\n",
    "from typing import Any, List, Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "import PIL\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from PIL import ImageOps\n",
    "from timm.data.constants import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
    "from timm.models.swin_transformer import SwinTransformer\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms.functional import resize, rotate\n",
    "from transformers import MBartConfig, MBartForCausalLM, XLMRobertaTokenizer\n",
    "from transformers.file_utils import ModelOutput\n",
    "from transformers.modeling_utils import PretrainedConfig, PreTrainedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-May-30 09:26:48.0600 498175:498175 ERROR  TDRV:tdrv_get_dev_info                       No neuron device available\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch_neuronx\n",
    "import os\n",
    "import numpy as np\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import sys\n",
    "\n",
    "import IPython\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from donut import DonutModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-05-30 09:32:39--  https://media.slidesgo.com/storage/162635/conversions/1-market-share-infographics-thumb.jpg\n",
      "Resolving media.slidesgo.com (media.slidesgo.com)... 104.22.1.146, 172.67.9.18, 104.22.0.146, ...\n",
      "Connecting to media.slidesgo.com (media.slidesgo.com)|104.22.1.146|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 43098 (42K) [image/jpeg]\n",
      "Saving to: ‘./test_01.jpg’\n",
      "\n",
      "./test_01.jpg       100%[===================>]  42.09K  --.-KB/s    in 0.001s  \n",
      "\n",
      "2023-05-30 09:32:39 (36.2 MB/s) - ‘./test_01.jpg’ saved [43098/43098]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!rm -rf test_01.jpg\n",
    "\n",
    "# 英語のスライド\n",
    "!wget -c https://media.slidesgo.com/storage/162635/conversions/1-market-share-infographics-thumb.jpg \\\n",
    "      -O ./test_01.jpg\n",
    "\n",
    "# 日本語のレシート\n",
    "# !wget -c https://www.isp21.co.jp/wp-content/uploads/solution/library/library02-1.jpg \\\n",
    "#       -O ./test_01.jpg\n",
    "\n",
    "input_img_path = './test_01.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████████████████████████████████████████████████████████████████████████████| 401/401 [00:00<00:00, 2.47MB/s]\n",
      "Downloading: 100%|████████████████████████████████████████████████████████████████████████████████| 965M/965M [00:10<00:00, 98.5MB/s]\n",
      "/home/ubuntu/aws_neuron_venv_pytorch/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "Downloading: 100%|██████████████████████████████████████████████████████████████████████████████| 1.24M/1.24M [00:00<00:00, 87.6MB/s]\n",
      "Downloading: 100%|██████████████████████████████████████████████████████████████████████████████████| 558/558 [00:00<00:00, 3.78MB/s]\n",
      "Downloading: 100%|██████████████████████████████████████████████████████████████████████████████████| 747/747 [00:00<00:00, 5.41MB/s]\n",
      "Downloading: 100%|██████████████████████████████████████████████████████████████████████████████████| 536/536 [00:00<00:00, 3.61MB/s]\n",
      "Downloading: 100%|██████████████████████████████████████████████████████████████████████████████| 3.83M/3.83M [00:00<00:00, 98.1MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DonutModel(\n",
       "  (encoder): SwinEncoder(\n",
       "    (model): SwinTransformer(\n",
       "      (patch_embed): PatchEmbed(\n",
       "        (proj): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "      (layers): Sequential(\n",
       "        (0): BasicLayer(\n",
       "          dim=128, input_resolution=(640, 480), depth=2\n",
       "          (blocks): ModuleList(\n",
       "            (0): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): Identity()\n",
       "              (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (1): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath()\n",
       "              (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (downsample): PatchMerging(\n",
       "            input_resolution=(640, 480), dim=128\n",
       "            (reduction): Linear(in_features=512, out_features=256, bias=False)\n",
       "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicLayer(\n",
       "          dim=256, input_resolution=(320, 240), depth=2\n",
       "          (blocks): ModuleList(\n",
       "            (0): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath()\n",
       "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (1): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath()\n",
       "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (downsample): PatchMerging(\n",
       "            input_resolution=(320, 240), dim=256\n",
       "            (reduction): Linear(in_features=1024, out_features=512, bias=False)\n",
       "            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (2): BasicLayer(\n",
       "          dim=512, input_resolution=(160, 120), depth=14\n",
       "          (blocks): ModuleList(\n",
       "            (0): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath()\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (1): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath()\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (2): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath()\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (3): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath()\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (4): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath()\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (5): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath()\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (6): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath()\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (7): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath()\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (8): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath()\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (9): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath()\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (10): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath()\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (11): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath()\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (12): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath()\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (13): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath()\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (downsample): PatchMerging(\n",
       "            input_resolution=(160, 120), dim=512\n",
       "            (reduction): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "            (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (3): BasicLayer(\n",
       "          dim=1024, input_resolution=(80, 60), depth=2\n",
       "          (blocks): ModuleList(\n",
       "            (0): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath()\n",
       "              (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (1): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath()\n",
       "              (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (avgpool): AdaptiveAvgPool1d(output_size=1)\n",
       "      (head): Identity()\n",
       "    )\n",
       "  )\n",
       "  (decoder): BARTDecoder(\n",
       "    (model): MBartForCausalLM(\n",
       "      (model): MBartDecoderWrapper(\n",
       "        (decoder): MBartDecoder(\n",
       "          (embed_tokens): Embedding(57544, 1024, padding_idx=1)\n",
       "          (embed_positions): MBartLearnedPositionalEmbedding(10, 1024)\n",
       "          (layers): ModuleList(\n",
       "            (0): MBartDecoderLayer(\n",
       "              (self_attn): MBartAttention(\n",
       "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (encoder_attn): MBartAttention(\n",
       "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (1): MBartDecoderLayer(\n",
       "              (self_attn): MBartAttention(\n",
       "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (encoder_attn): MBartAttention(\n",
       "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (2): MBartDecoderLayer(\n",
       "              (self_attn): MBartAttention(\n",
       "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (encoder_attn): MBartAttention(\n",
       "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (3): MBartDecoderLayer(\n",
       "              (self_attn): MBartAttention(\n",
       "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (encoder_attn): MBartAttention(\n",
       "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (lm_head): Linear(in_features=1024, out_features=57544, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "donut_model = DonutModel.from_pretrained(\"naver-clova-ix/donut-base-finetuned-rvlcdip\").to(\"cpu\")\n",
    "donut_model.eval() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_prompt = \"<s_rvlcdip>\"\n",
    "\n",
    "input_img = Image.open(\"./test_01.jpg\")\n",
    "output = donut_model.inference(image=input_img, prompt=task_prompt)[\"predictions\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'class': 'presentation'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timm.models.swin_transformer import WindowAttention, SwinTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_attention = WindowAttention(dim=1024, num_heads=32, window_size=(10, 10), qkv_bias=True, attn_drop=0.0, proj_drop=0.0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = torch.zeros([48, 100, 1024])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0068, -0.0451,  0.0154,  ..., -0.0066,  0.0085,  0.0078],\n",
       "         [-0.0068, -0.0451,  0.0154,  ..., -0.0066,  0.0085,  0.0078],\n",
       "         [-0.0068, -0.0451,  0.0154,  ..., -0.0066,  0.0085,  0.0078],\n",
       "         ...,\n",
       "         [-0.0068, -0.0451,  0.0154,  ..., -0.0066,  0.0085,  0.0078],\n",
       "         [-0.0068, -0.0451,  0.0154,  ..., -0.0066,  0.0085,  0.0078],\n",
       "         [-0.0068, -0.0451,  0.0154,  ..., -0.0066,  0.0085,  0.0078]],\n",
       "\n",
       "        [[-0.0068, -0.0451,  0.0154,  ..., -0.0066,  0.0085,  0.0078],\n",
       "         [-0.0068, -0.0451,  0.0154,  ..., -0.0066,  0.0085,  0.0078],\n",
       "         [-0.0068, -0.0451,  0.0154,  ..., -0.0066,  0.0085,  0.0078],\n",
       "         ...,\n",
       "         [-0.0068, -0.0451,  0.0154,  ..., -0.0066,  0.0085,  0.0078],\n",
       "         [-0.0068, -0.0451,  0.0154,  ..., -0.0066,  0.0085,  0.0078],\n",
       "         [-0.0068, -0.0451,  0.0154,  ..., -0.0066,  0.0085,  0.0078]],\n",
       "\n",
       "        [[-0.0068, -0.0451,  0.0154,  ..., -0.0066,  0.0085,  0.0078],\n",
       "         [-0.0068, -0.0451,  0.0154,  ..., -0.0066,  0.0085,  0.0078],\n",
       "         [-0.0068, -0.0451,  0.0154,  ..., -0.0066,  0.0085,  0.0078],\n",
       "         ...,\n",
       "         [-0.0068, -0.0451,  0.0154,  ..., -0.0066,  0.0085,  0.0078],\n",
       "         [-0.0068, -0.0451,  0.0154,  ..., -0.0066,  0.0085,  0.0078],\n",
       "         [-0.0068, -0.0451,  0.0154,  ..., -0.0066,  0.0085,  0.0078]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.0068, -0.0451,  0.0154,  ..., -0.0066,  0.0085,  0.0078],\n",
       "         [-0.0068, -0.0451,  0.0154,  ..., -0.0066,  0.0085,  0.0078],\n",
       "         [-0.0068, -0.0451,  0.0154,  ..., -0.0066,  0.0085,  0.0078],\n",
       "         ...,\n",
       "         [-0.0068, -0.0451,  0.0154,  ..., -0.0066,  0.0085,  0.0078],\n",
       "         [-0.0068, -0.0451,  0.0154,  ..., -0.0066,  0.0085,  0.0078],\n",
       "         [-0.0068, -0.0451,  0.0154,  ..., -0.0066,  0.0085,  0.0078]],\n",
       "\n",
       "        [[-0.0068, -0.0451,  0.0154,  ..., -0.0066,  0.0085,  0.0078],\n",
       "         [-0.0068, -0.0451,  0.0154,  ..., -0.0066,  0.0085,  0.0078],\n",
       "         [-0.0068, -0.0451,  0.0154,  ..., -0.0066,  0.0085,  0.0078],\n",
       "         ...,\n",
       "         [-0.0068, -0.0451,  0.0154,  ..., -0.0066,  0.0085,  0.0078],\n",
       "         [-0.0068, -0.0451,  0.0154,  ..., -0.0066,  0.0085,  0.0078],\n",
       "         [-0.0068, -0.0451,  0.0154,  ..., -0.0066,  0.0085,  0.0078]],\n",
       "\n",
       "        [[-0.0068, -0.0451,  0.0154,  ..., -0.0066,  0.0085,  0.0078],\n",
       "         [-0.0068, -0.0451,  0.0154,  ..., -0.0066,  0.0085,  0.0078],\n",
       "         [-0.0068, -0.0451,  0.0154,  ..., -0.0066,  0.0085,  0.0078],\n",
       "         ...,\n",
       "         [-0.0068, -0.0451,  0.0154,  ..., -0.0066,  0.0085,  0.0078],\n",
       "         [-0.0068, -0.0451,  0.0154,  ..., -0.0066,  0.0085,  0.0078],\n",
       "         [-0.0068, -0.0451,  0.0154,  ..., -0.0066,  0.0085,  0.0078]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window_attention.forward(dummy_input) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowAttention(\n",
       "  original_name=WindowAttention\n",
       "  (qkv): Linear(original_name=Linear)\n",
       "  (attn_drop): Dropout(original_name=Dropout)\n",
       "  (proj): Linear(original_name=Linear)\n",
       "  (proj_drop): Dropout(original_name=Dropout)\n",
       "  (softmax): Softmax(original_name=Softmax)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.jit.trace(window_attention, dummy_input) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuronModule(original_name=NeuronModule)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_neuronx.trace(window_attention, dummy_input) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timm.models.swin_transformer import SwinTransformerBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "stb = SwinTransformerBlock(dim=1024, input_resolution=(80, 60), num_heads=32, window_size=10, shift_size=5, mlp_ratio=4.0, qkv_bias=True, drop=0.0, attn_drop=0.0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = torch.zeros((1, 4800, 1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2110,  0.1001,  0.2346,  ...,  0.1103, -0.0052, -0.1115],\n",
       "         [-0.2110,  0.1001,  0.2346,  ...,  0.1103, -0.0052, -0.1115],\n",
       "         [-0.2110,  0.1001,  0.2346,  ...,  0.1103, -0.0052, -0.1115],\n",
       "         ...,\n",
       "         [-0.2110,  0.1001,  0.2346,  ...,  0.1103, -0.0052, -0.1115],\n",
       "         [-0.2110,  0.1001,  0.2346,  ...,  0.1103, -0.0052, -0.1115],\n",
       "         [-0.2110,  0.1001,  0.2346,  ...,  0.1103, -0.0052, -0.1115]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stb(dummy_input) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/aws_neuron_venv_pytorch/lib/python3.8/site-packages/torch/__init__.py:853: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert condition, message\n",
      "/home/ubuntu/aws_neuron_venv_pytorch/lib/python3.8/site-packages/timm/models/swin_transformer.py:119: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  B = int(windows.shape[0] / (H * W / window_size / window_size))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SwinTransformerBlock(\n",
       "  original_name=SwinTransformerBlock\n",
       "  (norm1): LayerNorm(original_name=LayerNorm)\n",
       "  (attn): WindowAttention(\n",
       "    original_name=WindowAttention\n",
       "    (qkv): Linear(original_name=Linear)\n",
       "    (attn_drop): Dropout(original_name=Dropout)\n",
       "    (proj): Linear(original_name=Linear)\n",
       "    (proj_drop): Dropout(original_name=Dropout)\n",
       "    (softmax): Softmax(original_name=Softmax)\n",
       "  )\n",
       "  (drop_path): Identity(original_name=Identity)\n",
       "  (norm2): LayerNorm(original_name=LayerNorm)\n",
       "  (mlp): Mlp(\n",
       "    original_name=Mlp\n",
       "    (fc1): Linear(original_name=Linear)\n",
       "    (act): GELU(original_name=GELU)\n",
       "    (drop1): Dropout(original_name=Dropout)\n",
       "    (fc2): Linear(original_name=Linear)\n",
       "    (drop2): Dropout(original_name=Dropout)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.jit.trace(stb, dummy_input) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuronModule(original_name=NeuronModule)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_neuronx.trace(stb, dummy_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "swin_transformer = SwinTransformer() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = torch.zeros((1, 3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0582e-01,  1.3340e-01, -1.2882e-01,  1.0457e+00, -2.0120e-01,\n",
       "          1.5382e-01, -6.9004e-01, -4.4204e-01, -2.1146e-02,  7.9767e-01,\n",
       "         -6.1969e-01, -3.8736e-01,  4.5748e-01,  2.0448e-01,  1.1823e+00,\n",
       "         -5.0816e-01,  1.1756e+00,  1.3605e-01, -2.7575e-01, -1.1183e-01,\n",
       "          6.5692e-01,  9.7905e-01, -4.2008e-01,  5.1391e-02, -3.2915e-01,\n",
       "          3.0060e-01, -1.1677e-01, -5.9148e-01,  4.1320e-01,  7.2589e-01,\n",
       "         -8.1697e-01,  5.4945e-02,  2.5633e-01,  8.1152e-01,  7.1605e-01,\n",
       "          8.2939e-01, -5.6233e-02,  2.9957e-01,  8.5922e-01, -7.0553e-01,\n",
       "          7.7346e-01,  4.3777e-01,  3.2396e-01, -9.0288e-02, -1.2218e+00,\n",
       "          6.8019e-01, -6.8559e-02,  4.8461e-02, -4.0518e-01,  3.7049e-01,\n",
       "          4.4140e-01, -1.6573e-01,  5.7697e-02,  6.8767e-01,  1.9585e-01,\n",
       "         -4.9692e-01, -9.7986e-02,  2.8836e-02, -8.2742e-01, -9.5816e-02,\n",
       "         -7.4876e-01, -7.7008e-01,  3.1343e-01, -7.6634e-01, -8.0548e-01,\n",
       "         -1.2685e-01,  3.1898e-01, -4.5712e-01, -2.6833e-01,  1.6410e-02,\n",
       "          4.6274e-01, -2.4482e-01,  4.1019e-01, -3.3255e-01, -6.4515e-01,\n",
       "          7.5254e-01, -4.9606e-01,  9.9051e-01,  6.3668e-01, -3.5311e-01,\n",
       "          9.2088e-02, -3.1177e-01, -5.5609e-01, -2.8102e-01,  7.9538e-02,\n",
       "          2.1307e-02,  1.2640e+00, -1.9101e-01, -4.3788e-01, -3.0646e-01,\n",
       "          1.4879e-01, -1.5770e-01, -1.6774e-01, -3.9267e-04, -2.6345e-01,\n",
       "         -3.6871e-01, -1.8652e-01,  3.9992e-01,  3.4283e-01,  5.2977e-02,\n",
       "         -3.9748e-01,  5.7532e-01, -6.7367e-01, -6.4942e-01, -7.2956e-03,\n",
       "          1.0418e-01, -4.4580e-01, -7.8040e-01, -2.0906e-01,  5.6426e-01,\n",
       "         -1.1201e+00, -1.0904e-01,  8.9112e-01, -5.2905e-02,  7.2798e-01,\n",
       "          1.2263e+00,  3.2611e-01, -4.6245e-01,  1.9963e-03, -5.7297e-01,\n",
       "          6.2391e-01,  1.1614e+00, -2.5565e-01, -6.2079e-01, -2.4671e-02,\n",
       "          9.3868e-02, -6.7095e-01,  5.3168e-01, -4.2105e-01, -3.6260e-01,\n",
       "         -5.3039e-01,  2.4315e-01,  6.8312e-02, -8.0322e-02,  4.1713e-02,\n",
       "         -4.3178e-01,  3.4485e-01,  1.3422e+00,  1.4804e+00,  9.1797e-01,\n",
       "          3.3859e-01,  1.6034e-01, -1.2162e-01, -5.4790e-01,  2.5450e-02,\n",
       "          9.0761e-01,  1.2281e+00,  3.6376e-01,  7.6525e-01, -2.8020e-01,\n",
       "         -6.8424e-01, -5.3963e-01,  1.8386e-01,  1.1795e+00,  1.7246e-01,\n",
       "          1.5853e-01,  4.3188e-01,  1.2147e+00,  1.6981e-01, -3.3840e-01,\n",
       "         -1.9191e-01,  8.7749e-01,  8.8326e-01,  5.7461e-01,  5.7034e-01,\n",
       "         -2.6977e-01, -1.4486e-01,  1.6868e-01,  4.0043e-01,  4.6528e-01,\n",
       "         -1.9118e-01,  6.0119e-01,  5.5035e-01,  2.5207e-01, -7.2923e-01,\n",
       "         -1.1699e+00, -3.8914e-02,  7.1236e-01,  8.5760e-01,  1.6728e-01,\n",
       "         -7.2925e-02,  3.8479e-01,  1.5290e-01, -7.9583e-01, -2.5171e-01,\n",
       "          4.3648e-01,  6.4594e-01, -3.3831e-01, -2.4646e-01,  6.0806e-02,\n",
       "          6.7155e-01,  7.3593e-01, -2.5944e-01,  8.4263e-01, -2.6443e-01,\n",
       "         -3.8781e-01, -5.4332e-01, -3.3898e-01,  4.0985e-01,  2.0327e-01,\n",
       "          5.9394e-01,  9.8467e-01,  1.5529e-01,  1.3047e+00,  4.0857e-01,\n",
       "         -6.6323e-02,  3.5164e-01,  8.2088e-01,  3.6389e-01,  3.2281e-01,\n",
       "         -1.1070e+00, -1.4713e-01,  7.5361e-01, -3.1218e-01,  1.5836e-02,\n",
       "          7.2539e-01,  9.3853e-01,  1.2166e-01,  1.4388e+00,  1.0179e+00,\n",
       "         -7.5808e-01,  3.7077e-01, -8.5284e-01, -1.8967e-01, -5.6169e-01,\n",
       "          1.5564e-01, -6.3770e-01, -2.2528e-01,  3.6581e-01, -4.1049e-02,\n",
       "          9.3875e-02, -7.7555e-01,  9.4287e-02, -5.3724e-01,  7.1034e-02,\n",
       "          1.2806e-01, -9.5345e-02, -5.7585e-01,  5.7419e-01,  3.5596e-01,\n",
       "          1.6039e-01, -2.3703e-01,  3.0618e-01, -1.3481e-01,  3.6757e-01,\n",
       "         -1.0968e+00,  2.5368e-03, -7.0440e-01, -3.3655e-01,  6.6051e-02,\n",
       "         -1.0420e+00,  1.0873e+00,  1.2001e+00,  8.6095e-01, -1.8814e-01,\n",
       "          7.6758e-01, -6.9131e-01,  8.3173e-01,  7.0382e-01, -6.0959e-01,\n",
       "         -7.3424e-01,  1.5175e-01, -1.9973e-01,  7.0286e-01,  4.8797e-01,\n",
       "         -1.3425e+00, -2.9771e-01,  3.6557e-01, -5.2509e-01,  8.5640e-02,\n",
       "          3.1795e-01, -5.5984e-01,  6.0599e-01, -7.3303e-01,  2.4844e-01,\n",
       "         -1.0425e+00,  8.0942e-01,  8.8078e-01,  3.0755e-02, -7.6411e-01,\n",
       "          6.9928e-01,  4.1662e-01, -9.7431e-01,  4.6090e-01, -7.2063e-01,\n",
       "          1.9545e-01,  2.1900e-01,  5.2444e-02,  4.2967e-01,  4.5139e-01,\n",
       "          8.0351e-01,  2.7543e-01,  3.7730e-01, -1.5088e-01,  1.7646e-01,\n",
       "         -8.2170e-01, -1.4640e-01, -1.4245e-01,  1.7546e+00,  8.6161e-02,\n",
       "          1.1916e+00,  5.2105e-01, -3.8506e-01, -7.0860e-02, -7.6802e-01,\n",
       "         -8.3157e-01, -4.8292e-02,  4.6617e-01, -5.0361e-01, -6.9993e-02,\n",
       "          7.4365e-01, -4.7446e-01,  3.7616e-01, -6.8063e-01,  3.7934e-01,\n",
       "         -9.3080e-02, -3.0066e-01,  1.8880e-01, -1.8945e-01,  1.1463e+00,\n",
       "         -2.7832e-02, -1.0137e+00,  4.7862e-01, -5.0541e-01, -1.0043e-01,\n",
       "         -9.1792e-01,  9.2505e-01,  1.9829e-01, -2.8357e-01, -4.7623e-01,\n",
       "         -6.6222e-01, -7.3086e-02,  1.6606e-02, -1.3834e+00,  3.6128e-01,\n",
       "         -2.5945e-01, -1.4487e-03, -3.7985e-01, -3.8431e-01,  1.0412e-02,\n",
       "         -1.1581e+00,  1.2504e+00,  2.4839e-01,  4.7310e-02, -4.8318e-01,\n",
       "          2.8478e-01, -7.1138e-01, -7.6037e-01,  1.5816e-01, -1.0762e+00,\n",
       "         -1.4817e-01,  8.7088e-02,  3.6224e-01, -4.0471e-01,  5.9696e-01,\n",
       "          2.5261e-01,  1.0589e+00,  3.7478e-01,  4.1257e-01,  1.8178e-01,\n",
       "          6.6300e-01, -9.5131e-01, -1.2592e+00, -4.6909e-01, -5.3234e-02,\n",
       "          3.4119e-01,  3.5585e-01,  1.4660e-01,  7.8497e-01,  5.4672e-01,\n",
       "          5.1435e-01, -3.6138e-01,  9.4989e-01,  3.4532e-01, -9.9236e-01,\n",
       "          1.5722e-01,  1.1091e+00,  3.2596e-01, -1.0076e+00,  2.2521e-01,\n",
       "          8.9392e-01,  1.7304e-01,  4.1327e-01,  3.4359e-01,  1.8216e-02,\n",
       "         -9.6400e-01,  1.7366e-01,  7.2710e-02,  5.4092e-01,  1.1777e-01,\n",
       "         -5.9422e-01,  4.0978e-01,  5.3799e-02, -5.1353e-01,  9.6173e-02,\n",
       "          4.3366e-01, -8.2697e-01,  7.5650e-04, -1.1407e-01,  1.6710e-01,\n",
       "         -3.3980e-01,  1.2498e-01, -7.1640e-01,  2.0754e-01,  5.2502e-01,\n",
       "          2.5695e-02,  1.3443e-03, -2.2698e-01, -5.9457e-02,  1.9254e-01,\n",
       "          2.1547e-01, -1.3295e-01,  1.0702e-01, -3.3988e-01, -6.1540e-02,\n",
       "          4.5422e-01,  6.8331e-03, -2.3299e-01,  4.0691e-01,  4.0280e-01,\n",
       "         -4.2748e-01, -7.9079e-01, -4.9652e-01,  4.2589e-01, -1.0248e+00,\n",
       "         -7.0404e-01, -4.2791e-01,  7.1882e-01, -4.9317e-01, -1.0391e+00,\n",
       "          8.2931e-02,  5.6060e-01,  3.1246e-01, -8.7931e-01, -6.5437e-01,\n",
       "          1.8879e-01, -9.7378e-01,  7.2959e-01, -3.1271e-01,  3.2746e-01,\n",
       "         -2.7599e-01,  4.2020e-01, -5.0943e-01,  3.2953e-01, -1.3930e-01,\n",
       "          3.6335e-01, -1.3103e-01, -6.5487e-01,  7.8109e-01,  8.5990e-01,\n",
       "         -7.9997e-01,  1.5076e-01, -5.7691e-02,  1.4554e-02, -2.0092e-03,\n",
       "         -4.1411e-01,  7.1748e-01, -1.1561e-01,  4.8665e-01, -4.3911e-01,\n",
       "         -5.3290e-01,  7.6264e-02, -1.0842e-01, -5.8733e-01, -2.9564e-03,\n",
       "         -7.7141e-01,  2.8577e-01,  3.2501e-01,  9.0943e-02, -2.4885e-01,\n",
       "         -3.6346e-01, -4.1972e-01, -3.9492e-03, -5.7732e-01, -7.5151e-01,\n",
       "         -3.1607e-01,  5.0548e-01, -2.0713e-01, -3.1196e-01,  1.4775e-01,\n",
       "          1.5227e-01, -7.7548e-02,  5.4546e-01,  1.2382e-01, -5.4213e-01,\n",
       "          6.3036e-01,  3.7103e-01,  5.5089e-01, -5.5402e-01, -2.6884e-01,\n",
       "          3.5563e-01,  6.5985e-01, -3.5518e-01,  3.1558e-01, -1.1016e-01,\n",
       "          7.0596e-01,  8.9219e-01, -1.7119e-01,  3.5932e-01,  6.7906e-02,\n",
       "          1.0881e-01, -2.0211e-01,  3.3429e-01,  5.4042e-01, -1.9568e-01,\n",
       "         -4.1337e-01,  9.7122e-02, -1.4707e-01, -1.0688e+00, -1.9828e-01,\n",
       "         -5.6461e-01,  1.3234e-01, -1.0704e+00, -6.5151e-01,  1.6106e-01,\n",
       "         -6.0537e-01,  2.1149e-01,  6.6257e-02,  3.4327e-01, -6.4001e-02,\n",
       "         -1.0375e-01, -3.1199e-01,  1.3062e-01,  1.6480e-01,  1.3364e+00,\n",
       "          8.5710e-01, -4.2802e-01,  4.9959e-01, -4.3340e-01,  3.9074e-01,\n",
       "          6.7827e-01,  3.7559e-01,  1.7215e+00,  2.4461e-01, -7.1631e-01,\n",
       "          4.4208e-01,  7.5737e-01,  1.0821e+00,  7.6470e-01, -3.4753e-01,\n",
       "          3.5106e-01, -2.9969e-01,  4.2870e-01, -7.2987e-01, -4.4608e-02,\n",
       "         -1.3612e-01,  6.8939e-01, -6.1937e-01,  5.3169e-01, -8.3811e-02,\n",
       "         -1.2529e+00, -7.2126e-01, -2.0198e+00,  9.1167e-02,  1.9648e-01,\n",
       "          5.2837e-01,  3.2529e-01,  1.0086e+00,  4.6605e-02, -6.8452e-02,\n",
       "         -4.2153e-01,  6.3600e-02, -5.7385e-01,  6.2948e-01,  1.7872e-01,\n",
       "         -3.5523e-01,  1.6058e-01, -1.0629e+00,  7.8202e-03,  1.0213e+00,\n",
       "          2.3059e-01, -8.5481e-01, -3.9154e-01, -3.6787e-01, -3.4246e-01,\n",
       "         -2.5196e-01, -8.0299e-01, -7.3609e-01,  8.3926e-01,  5.4495e-01,\n",
       "         -2.4650e-01, -3.0383e-01,  9.9012e-02, -7.0245e-02, -9.7904e-01,\n",
       "          4.2868e-01,  6.3290e-01,  8.0622e-01, -7.4764e-01, -1.0234e-01,\n",
       "         -4.1937e-01,  3.5990e-01, -3.7169e-01, -4.1882e-02, -1.6980e-01,\n",
       "          7.5245e-01,  4.2304e-01,  6.7480e-01, -4.4807e-01,  4.5110e-01,\n",
       "          1.1166e-01,  8.3151e-01,  3.0642e-01, -5.2896e-01, -4.3449e-03,\n",
       "         -2.4937e-01,  1.7859e+00,  2.6184e-01,  7.0141e-01,  3.2574e-01,\n",
       "         -4.0760e-02,  3.3762e-01, -3.3904e-01,  6.6436e-02, -7.3167e-01,\n",
       "         -1.7666e-01,  9.5680e-01,  1.0623e-01, -3.7591e-01,  1.1339e+00,\n",
       "         -2.1586e-01, -2.2787e-01,  3.0407e-01,  7.3746e-01, -4.3080e-02,\n",
       "         -1.3693e+00,  1.9831e-01, -5.9249e-01, -4.5142e-01, -4.0731e-01,\n",
       "         -6.5325e-01,  6.1633e-02, -1.6854e-01, -1.1370e-01, -4.6648e-03,\n",
       "         -5.1070e-01, -5.5088e-01,  2.1169e-02, -7.8923e-01, -2.0711e-01,\n",
       "          7.4005e-01,  3.7521e-01,  6.3363e-01,  4.4856e-01, -4.5887e-01,\n",
       "         -7.1556e-02, -2.5882e-01,  9.6946e-02, -1.3146e-01,  4.0861e-01,\n",
       "         -7.2067e-01, -2.5690e-01, -3.8054e-02,  3.5647e-01,  6.4813e-01,\n",
       "          3.7720e-01,  3.1606e-01, -2.8396e-01,  9.4470e-01,  6.4188e-01,\n",
       "          1.2216e-01, -1.0777e-02, -1.6229e-01,  5.1929e-02, -2.8490e-01,\n",
       "         -1.2953e+00, -4.3577e-01, -2.2599e-01,  1.9606e-01, -6.5701e-01,\n",
       "          4.3082e-02,  3.6119e-01,  9.1074e-01,  3.5372e-01, -5.0555e-01,\n",
       "          5.9045e-01,  7.6789e-01,  2.5424e-01,  1.1191e+00,  1.2523e-01,\n",
       "         -7.4355e-01,  1.4899e-01, -5.1455e-03, -3.3878e-01, -1.3340e-01,\n",
       "          5.8950e-01,  3.8023e-01, -6.3262e-01, -1.3432e-01,  3.7684e-01,\n",
       "          5.3590e-01, -4.8343e-01, -2.9014e-01,  7.6815e-02, -2.2607e-01,\n",
       "          2.2242e-01,  3.5931e-02,  6.4918e-01, -7.9457e-01, -1.9968e-01,\n",
       "          5.8557e-01, -6.1155e-01, -3.6490e-01, -2.3798e-01,  2.2317e-01,\n",
       "         -7.8849e-01,  1.0843e-02,  5.8062e-01,  1.5673e-01, -2.9442e-01,\n",
       "         -7.8633e-01, -3.8595e-02,  1.1686e+00, -8.1867e-01,  3.5571e-01,\n",
       "          2.3014e-01, -4.0527e-01,  3.4720e-01, -3.1785e-01, -1.8893e-01,\n",
       "          7.2787e-03,  1.8646e-02,  5.5646e-01, -4.8944e-01, -1.4926e+00,\n",
       "          8.4507e-01,  1.4650e-01,  1.2752e-02, -3.1365e-01,  5.3893e-01,\n",
       "         -7.9519e-02,  3.2736e-01,  4.5279e-01,  6.1143e-02, -1.8087e-01,\n",
       "          4.7542e-01, -1.5022e-01, -3.0072e-01, -1.4929e+00, -2.0354e-01,\n",
       "         -3.5760e-01,  1.0387e+00,  8.8783e-02, -1.1351e+00, -1.1649e-01,\n",
       "         -1.0458e-01, -8.3323e-01, -1.0522e-01,  1.1692e+00,  4.3836e-01,\n",
       "         -4.0856e-01, -4.4653e-02,  2.5262e-02,  4.0481e-02,  1.8938e-02,\n",
       "         -7.7967e-01,  9.1556e-01, -1.0113e+00, -3.2746e-01,  2.8219e-01,\n",
       "          8.4369e-01, -5.7993e-01, -2.4104e-02,  3.1661e-02, -5.0584e-01,\n",
       "          4.9274e-01, -1.0032e+00, -7.9566e-02,  3.8695e-01,  1.9702e-02,\n",
       "         -5.5986e-02, -7.4952e-01, -4.1338e-01,  2.9660e-01, -6.2326e-01,\n",
       "         -3.1767e-01,  1.5630e+00,  7.6901e-01,  1.8709e-03,  1.4304e-01,\n",
       "         -4.7641e-01, -7.1829e-01,  9.3645e-01, -6.6236e-01,  5.6273e-01,\n",
       "         -6.2387e-02, -6.8450e-01,  1.4558e-01, -2.3351e-01, -2.8551e-01,\n",
       "          5.4677e-01,  4.3192e-01, -4.2111e-01, -1.3110e-01, -2.3906e-01,\n",
       "         -1.1138e+00, -4.1364e-01, -2.9704e-01, -1.2758e-01, -6.5918e-01,\n",
       "         -9.3963e-01,  3.8181e-01, -4.1560e-01,  5.2001e-01,  5.0268e-01,\n",
       "          5.7636e-01, -3.4545e-01, -7.7569e-01, -8.5241e-01,  8.0887e-01,\n",
       "         -5.0308e-01,  2.4237e-01, -2.5607e-01,  6.7861e-01, -5.4448e-01,\n",
       "         -1.2959e-01, -3.3551e-02,  7.0688e-01, -2.1476e-01, -2.4324e-01,\n",
       "          8.7857e-01, -1.3994e-01, -3.0878e-01, -3.7030e-01,  5.3885e-01,\n",
       "          2.5616e-01, -4.0637e-01, -1.5220e-01, -4.9157e-01,  9.0158e-02,\n",
       "          5.7507e-01, -4.3972e-01, -3.0302e-01,  2.1850e-01, -5.5135e-01,\n",
       "          9.2320e-01, -4.9231e-01,  1.0015e+00, -2.3344e-01,  9.4971e-02,\n",
       "          5.3964e-03,  2.3477e-01, -3.2091e-01, -5.0496e-01,  1.1662e+00,\n",
       "         -3.0027e-01, -6.7832e-01, -2.3145e-01,  1.9333e-01,  1.1126e+00,\n",
       "         -2.8740e-01,  9.8930e-02, -3.5344e-01,  5.4874e-03, -5.9615e-01,\n",
       "         -2.9196e-01, -3.0515e-01, -6.6144e-01, -1.4456e-01,  9.9900e-01,\n",
       "          1.4129e-01, -5.5059e-02,  3.9117e-01, -1.0653e+00, -2.3402e-01,\n",
       "          7.9969e-01,  1.4301e-01,  8.1654e-02, -8.9677e-01, -3.8976e-01,\n",
       "          2.4793e-01,  9.6244e-01,  1.8921e-01,  1.3193e+00,  1.1155e+00,\n",
       "          4.4691e-01,  1.4627e+00, -1.2097e-01,  3.7656e-01, -2.2710e-01,\n",
       "          9.9312e-02,  1.7949e-01, -1.4839e-01,  8.9317e-01, -7.1851e-01,\n",
       "          5.0285e-01,  1.2601e+00, -3.9803e-01,  2.0643e-01, -3.3004e-01,\n",
       "          8.7699e-01,  9.9474e-01, -6.4950e-01, -3.7398e-01,  5.3724e-01,\n",
       "          7.9499e-02, -9.1536e-01, -5.6080e-01, -1.9173e-01,  1.1651e-01,\n",
       "          8.3349e-01, -3.2406e-01, -4.2223e-01, -3.0406e-01,  1.5857e-01,\n",
       "         -6.0376e-01, -7.2351e-02, -1.8205e-02,  2.4447e-01,  4.1125e-01,\n",
       "         -8.0582e-01, -1.3892e+00,  5.0339e-01, -1.0313e-01,  5.4704e-03,\n",
       "         -3.1421e-01,  3.6012e-01,  2.9293e-01, -7.0141e-02, -1.8500e-01,\n",
       "         -1.4639e-01, -5.2579e-01, -3.1133e-01,  2.7231e-01,  5.5391e-01,\n",
       "          7.3302e-01, -1.9870e-01,  1.5189e-02, -6.9669e-01,  7.5633e-01,\n",
       "         -6.4348e-01, -1.7178e-01,  2.7544e-01,  1.0831e+00,  2.3404e-01,\n",
       "          4.2390e-01,  5.8357e-01, -1.3324e+00,  2.8630e-01, -7.9147e-03,\n",
       "          8.1116e-02,  2.6958e-01,  3.2275e-01,  8.2845e-02,  6.2066e-01,\n",
       "          1.6239e-01,  4.7683e-01,  5.4490e-01,  1.2483e-01,  5.8469e-01,\n",
       "          5.0529e-01,  7.8345e-01, -9.5780e-01, -3.1449e-01, -3.9852e-01,\n",
       "         -7.0384e-01, -4.3818e-01,  4.2288e-01, -3.8934e-01, -2.7909e-01,\n",
       "         -1.7793e-01,  7.1402e-01, -3.4066e-01, -1.2071e+00,  7.4893e-01,\n",
       "         -3.4856e-01,  6.8344e-02, -7.4434e-01, -1.2432e-01, -1.1567e-01,\n",
       "          7.9763e-02, -4.4150e-01,  3.1926e-01,  8.3574e-02, -1.6584e-01,\n",
       "          2.1508e-01,  6.6011e-01,  7.2488e-01,  6.1238e-01, -1.5730e-01,\n",
       "          9.0397e-01,  3.3592e-01, -2.0602e-01, -5.9633e-01,  5.0421e-01,\n",
       "          2.8912e-01, -3.4885e-01, -6.1400e-01, -3.6160e-01,  2.0686e-01,\n",
       "          7.3889e-02,  3.7383e-01,  6.1255e-01,  1.1587e+00, -1.8896e-01,\n",
       "         -2.4606e-01, -7.6329e-01, -3.5180e-02,  1.5049e-01, -7.9952e-01]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swin_transformer(dummy_input) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/aws_neuron_venv_pytorch/lib/python3.8/site-packages/torch/jit/_trace.py:1001: TracerWarning: Output nr 1. of the traced function does not match the corresponding output of the Python function. Detailed error:\n",
      "Tensor-likes are not close!\n",
      "\n",
      "Mismatched elements: 1000 / 1000 (100.0%)\n",
      "Greatest absolute difference: 0.9971470087766647 at index (0, 5) (up to 1e-05 allowed)\n",
      "Greatest relative difference: 2493.6227840690804 at index (0, 296) (up to 1e-05 allowed)\n",
      "  _check_trace(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SwinTransformer(\n",
       "  original_name=SwinTransformer\n",
       "  (patch_embed): PatchEmbed(\n",
       "    original_name=PatchEmbed\n",
       "    (proj): Conv2d(original_name=Conv2d)\n",
       "    (norm): LayerNorm(original_name=LayerNorm)\n",
       "  )\n",
       "  (pos_drop): Dropout(original_name=Dropout)\n",
       "  (layers): Sequential(\n",
       "    original_name=Sequential\n",
       "    (0): BasicLayer(\n",
       "      original_name=BasicLayer\n",
       "      (blocks): ModuleList(\n",
       "        original_name=ModuleList\n",
       "        (0): SwinTransformerBlock(\n",
       "          original_name=SwinTransformerBlock\n",
       "          (norm1): LayerNorm(original_name=LayerNorm)\n",
       "          (attn): WindowAttention(\n",
       "            original_name=WindowAttention\n",
       "            (qkv): Linear(original_name=Linear)\n",
       "            (attn_drop): Dropout(original_name=Dropout)\n",
       "            (proj): Linear(original_name=Linear)\n",
       "            (proj_drop): Dropout(original_name=Dropout)\n",
       "            (softmax): Softmax(original_name=Softmax)\n",
       "          )\n",
       "          (drop_path): Identity(original_name=Identity)\n",
       "          (norm2): LayerNorm(original_name=LayerNorm)\n",
       "          (mlp): Mlp(\n",
       "            original_name=Mlp\n",
       "            (fc1): Linear(original_name=Linear)\n",
       "            (act): GELU(original_name=GELU)\n",
       "            (drop1): Dropout(original_name=Dropout)\n",
       "            (fc2): Linear(original_name=Linear)\n",
       "            (drop2): Dropout(original_name=Dropout)\n",
       "          )\n",
       "        )\n",
       "        (1): SwinTransformerBlock(\n",
       "          original_name=SwinTransformerBlock\n",
       "          (norm1): LayerNorm(original_name=LayerNorm)\n",
       "          (attn): WindowAttention(\n",
       "            original_name=WindowAttention\n",
       "            (qkv): Linear(original_name=Linear)\n",
       "            (attn_drop): Dropout(original_name=Dropout)\n",
       "            (proj): Linear(original_name=Linear)\n",
       "            (proj_drop): Dropout(original_name=Dropout)\n",
       "            (softmax): Softmax(original_name=Softmax)\n",
       "          )\n",
       "          (drop_path): DropPath(original_name=DropPath)\n",
       "          (norm2): LayerNorm(original_name=LayerNorm)\n",
       "          (mlp): Mlp(\n",
       "            original_name=Mlp\n",
       "            (fc1): Linear(original_name=Linear)\n",
       "            (act): GELU(original_name=GELU)\n",
       "            (drop1): Dropout(original_name=Dropout)\n",
       "            (fc2): Linear(original_name=Linear)\n",
       "            (drop2): Dropout(original_name=Dropout)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (downsample): PatchMerging(\n",
       "        original_name=PatchMerging\n",
       "        (reduction): Linear(original_name=Linear)\n",
       "        (norm): LayerNorm(original_name=LayerNorm)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicLayer(\n",
       "      original_name=BasicLayer\n",
       "      (blocks): ModuleList(\n",
       "        original_name=ModuleList\n",
       "        (0): SwinTransformerBlock(\n",
       "          original_name=SwinTransformerBlock\n",
       "          (norm1): LayerNorm(original_name=LayerNorm)\n",
       "          (attn): WindowAttention(\n",
       "            original_name=WindowAttention\n",
       "            (qkv): Linear(original_name=Linear)\n",
       "            (attn_drop): Dropout(original_name=Dropout)\n",
       "            (proj): Linear(original_name=Linear)\n",
       "            (proj_drop): Dropout(original_name=Dropout)\n",
       "            (softmax): Softmax(original_name=Softmax)\n",
       "          )\n",
       "          (drop_path): DropPath(original_name=DropPath)\n",
       "          (norm2): LayerNorm(original_name=LayerNorm)\n",
       "          (mlp): Mlp(\n",
       "            original_name=Mlp\n",
       "            (fc1): Linear(original_name=Linear)\n",
       "            (act): GELU(original_name=GELU)\n",
       "            (drop1): Dropout(original_name=Dropout)\n",
       "            (fc2): Linear(original_name=Linear)\n",
       "            (drop2): Dropout(original_name=Dropout)\n",
       "          )\n",
       "        )\n",
       "        (1): SwinTransformerBlock(\n",
       "          original_name=SwinTransformerBlock\n",
       "          (norm1): LayerNorm(original_name=LayerNorm)\n",
       "          (attn): WindowAttention(\n",
       "            original_name=WindowAttention\n",
       "            (qkv): Linear(original_name=Linear)\n",
       "            (attn_drop): Dropout(original_name=Dropout)\n",
       "            (proj): Linear(original_name=Linear)\n",
       "            (proj_drop): Dropout(original_name=Dropout)\n",
       "            (softmax): Softmax(original_name=Softmax)\n",
       "          )\n",
       "          (drop_path): DropPath(original_name=DropPath)\n",
       "          (norm2): LayerNorm(original_name=LayerNorm)\n",
       "          (mlp): Mlp(\n",
       "            original_name=Mlp\n",
       "            (fc1): Linear(original_name=Linear)\n",
       "            (act): GELU(original_name=GELU)\n",
       "            (drop1): Dropout(original_name=Dropout)\n",
       "            (fc2): Linear(original_name=Linear)\n",
       "            (drop2): Dropout(original_name=Dropout)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (downsample): PatchMerging(\n",
       "        original_name=PatchMerging\n",
       "        (reduction): Linear(original_name=Linear)\n",
       "        (norm): LayerNorm(original_name=LayerNorm)\n",
       "      )\n",
       "    )\n",
       "    (2): BasicLayer(\n",
       "      original_name=BasicLayer\n",
       "      (blocks): ModuleList(\n",
       "        original_name=ModuleList\n",
       "        (0): SwinTransformerBlock(\n",
       "          original_name=SwinTransformerBlock\n",
       "          (norm1): LayerNorm(original_name=LayerNorm)\n",
       "          (attn): WindowAttention(\n",
       "            original_name=WindowAttention\n",
       "            (qkv): Linear(original_name=Linear)\n",
       "            (attn_drop): Dropout(original_name=Dropout)\n",
       "            (proj): Linear(original_name=Linear)\n",
       "            (proj_drop): Dropout(original_name=Dropout)\n",
       "            (softmax): Softmax(original_name=Softmax)\n",
       "          )\n",
       "          (drop_path): DropPath(original_name=DropPath)\n",
       "          (norm2): LayerNorm(original_name=LayerNorm)\n",
       "          (mlp): Mlp(\n",
       "            original_name=Mlp\n",
       "            (fc1): Linear(original_name=Linear)\n",
       "            (act): GELU(original_name=GELU)\n",
       "            (drop1): Dropout(original_name=Dropout)\n",
       "            (fc2): Linear(original_name=Linear)\n",
       "            (drop2): Dropout(original_name=Dropout)\n",
       "          )\n",
       "        )\n",
       "        (1): SwinTransformerBlock(\n",
       "          original_name=SwinTransformerBlock\n",
       "          (norm1): LayerNorm(original_name=LayerNorm)\n",
       "          (attn): WindowAttention(\n",
       "            original_name=WindowAttention\n",
       "            (qkv): Linear(original_name=Linear)\n",
       "            (attn_drop): Dropout(original_name=Dropout)\n",
       "            (proj): Linear(original_name=Linear)\n",
       "            (proj_drop): Dropout(original_name=Dropout)\n",
       "            (softmax): Softmax(original_name=Softmax)\n",
       "          )\n",
       "          (drop_path): DropPath(original_name=DropPath)\n",
       "          (norm2): LayerNorm(original_name=LayerNorm)\n",
       "          (mlp): Mlp(\n",
       "            original_name=Mlp\n",
       "            (fc1): Linear(original_name=Linear)\n",
       "            (act): GELU(original_name=GELU)\n",
       "            (drop1): Dropout(original_name=Dropout)\n",
       "            (fc2): Linear(original_name=Linear)\n",
       "            (drop2): Dropout(original_name=Dropout)\n",
       "          )\n",
       "        )\n",
       "        (2): SwinTransformerBlock(\n",
       "          original_name=SwinTransformerBlock\n",
       "          (norm1): LayerNorm(original_name=LayerNorm)\n",
       "          (attn): WindowAttention(\n",
       "            original_name=WindowAttention\n",
       "            (qkv): Linear(original_name=Linear)\n",
       "            (attn_drop): Dropout(original_name=Dropout)\n",
       "            (proj): Linear(original_name=Linear)\n",
       "            (proj_drop): Dropout(original_name=Dropout)\n",
       "            (softmax): Softmax(original_name=Softmax)\n",
       "          )\n",
       "          (drop_path): DropPath(original_name=DropPath)\n",
       "          (norm2): LayerNorm(original_name=LayerNorm)\n",
       "          (mlp): Mlp(\n",
       "            original_name=Mlp\n",
       "            (fc1): Linear(original_name=Linear)\n",
       "            (act): GELU(original_name=GELU)\n",
       "            (drop1): Dropout(original_name=Dropout)\n",
       "            (fc2): Linear(original_name=Linear)\n",
       "            (drop2): Dropout(original_name=Dropout)\n",
       "          )\n",
       "        )\n",
       "        (3): SwinTransformerBlock(\n",
       "          original_name=SwinTransformerBlock\n",
       "          (norm1): LayerNorm(original_name=LayerNorm)\n",
       "          (attn): WindowAttention(\n",
       "            original_name=WindowAttention\n",
       "            (qkv): Linear(original_name=Linear)\n",
       "            (attn_drop): Dropout(original_name=Dropout)\n",
       "            (proj): Linear(original_name=Linear)\n",
       "            (proj_drop): Dropout(original_name=Dropout)\n",
       "            (softmax): Softmax(original_name=Softmax)\n",
       "          )\n",
       "          (drop_path): DropPath(original_name=DropPath)\n",
       "          (norm2): LayerNorm(original_name=LayerNorm)\n",
       "          (mlp): Mlp(\n",
       "            original_name=Mlp\n",
       "            (fc1): Linear(original_name=Linear)\n",
       "            (act): GELU(original_name=GELU)\n",
       "            (drop1): Dropout(original_name=Dropout)\n",
       "            (fc2): Linear(original_name=Linear)\n",
       "            (drop2): Dropout(original_name=Dropout)\n",
       "          )\n",
       "        )\n",
       "        (4): SwinTransformerBlock(\n",
       "          original_name=SwinTransformerBlock\n",
       "          (norm1): LayerNorm(original_name=LayerNorm)\n",
       "          (attn): WindowAttention(\n",
       "            original_name=WindowAttention\n",
       "            (qkv): Linear(original_name=Linear)\n",
       "            (attn_drop): Dropout(original_name=Dropout)\n",
       "            (proj): Linear(original_name=Linear)\n",
       "            (proj_drop): Dropout(original_name=Dropout)\n",
       "            (softmax): Softmax(original_name=Softmax)\n",
       "          )\n",
       "          (drop_path): DropPath(original_name=DropPath)\n",
       "          (norm2): LayerNorm(original_name=LayerNorm)\n",
       "          (mlp): Mlp(\n",
       "            original_name=Mlp\n",
       "            (fc1): Linear(original_name=Linear)\n",
       "            (act): GELU(original_name=GELU)\n",
       "            (drop1): Dropout(original_name=Dropout)\n",
       "            (fc2): Linear(original_name=Linear)\n",
       "            (drop2): Dropout(original_name=Dropout)\n",
       "          )\n",
       "        )\n",
       "        (5): SwinTransformerBlock(\n",
       "          original_name=SwinTransformerBlock\n",
       "          (norm1): LayerNorm(original_name=LayerNorm)\n",
       "          (attn): WindowAttention(\n",
       "            original_name=WindowAttention\n",
       "            (qkv): Linear(original_name=Linear)\n",
       "            (attn_drop): Dropout(original_name=Dropout)\n",
       "            (proj): Linear(original_name=Linear)\n",
       "            (proj_drop): Dropout(original_name=Dropout)\n",
       "            (softmax): Softmax(original_name=Softmax)\n",
       "          )\n",
       "          (drop_path): DropPath(original_name=DropPath)\n",
       "          (norm2): LayerNorm(original_name=LayerNorm)\n",
       "          (mlp): Mlp(\n",
       "            original_name=Mlp\n",
       "            (fc1): Linear(original_name=Linear)\n",
       "            (act): GELU(original_name=GELU)\n",
       "            (drop1): Dropout(original_name=Dropout)\n",
       "            (fc2): Linear(original_name=Linear)\n",
       "            (drop2): Dropout(original_name=Dropout)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (downsample): PatchMerging(\n",
       "        original_name=PatchMerging\n",
       "        (reduction): Linear(original_name=Linear)\n",
       "        (norm): LayerNorm(original_name=LayerNorm)\n",
       "      )\n",
       "    )\n",
       "    (3): BasicLayer(\n",
       "      original_name=BasicLayer\n",
       "      (blocks): ModuleList(\n",
       "        original_name=ModuleList\n",
       "        (0): SwinTransformerBlock(\n",
       "          original_name=SwinTransformerBlock\n",
       "          (norm1): LayerNorm(original_name=LayerNorm)\n",
       "          (attn): WindowAttention(\n",
       "            original_name=WindowAttention\n",
       "            (qkv): Linear(original_name=Linear)\n",
       "            (attn_drop): Dropout(original_name=Dropout)\n",
       "            (proj): Linear(original_name=Linear)\n",
       "            (proj_drop): Dropout(original_name=Dropout)\n",
       "            (softmax): Softmax(original_name=Softmax)\n",
       "          )\n",
       "          (drop_path): DropPath(original_name=DropPath)\n",
       "          (norm2): LayerNorm(original_name=LayerNorm)\n",
       "          (mlp): Mlp(\n",
       "            original_name=Mlp\n",
       "            (fc1): Linear(original_name=Linear)\n",
       "            (act): GELU(original_name=GELU)\n",
       "            (drop1): Dropout(original_name=Dropout)\n",
       "            (fc2): Linear(original_name=Linear)\n",
       "            (drop2): Dropout(original_name=Dropout)\n",
       "          )\n",
       "        )\n",
       "        (1): SwinTransformerBlock(\n",
       "          original_name=SwinTransformerBlock\n",
       "          (norm1): LayerNorm(original_name=LayerNorm)\n",
       "          (attn): WindowAttention(\n",
       "            original_name=WindowAttention\n",
       "            (qkv): Linear(original_name=Linear)\n",
       "            (attn_drop): Dropout(original_name=Dropout)\n",
       "            (proj): Linear(original_name=Linear)\n",
       "            (proj_drop): Dropout(original_name=Dropout)\n",
       "            (softmax): Softmax(original_name=Softmax)\n",
       "          )\n",
       "          (drop_path): DropPath(original_name=DropPath)\n",
       "          (norm2): LayerNorm(original_name=LayerNorm)\n",
       "          (mlp): Mlp(\n",
       "            original_name=Mlp\n",
       "            (fc1): Linear(original_name=Linear)\n",
       "            (act): GELU(original_name=GELU)\n",
       "            (drop1): Dropout(original_name=Dropout)\n",
       "            (fc2): Linear(original_name=Linear)\n",
       "            (drop2): Dropout(original_name=Dropout)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm(original_name=LayerNorm)\n",
       "  (avgpool): AdaptiveAvgPool1d(original_name=AdaptiveAvgPool1d)\n",
       "  (head): Linear(original_name=Linear)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.jit.trace(swin_transformer, dummy_input) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "swin_transformer_neuron = torch_neuronx.trace(swin_transformer, dummy_input) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuron 3.8.10",
   "language": "python",
   "name": "aws_neuron_venv_pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
